%
\section{Multilinear Algebra}
%
Tensors fields will play an important role in thr study of manifolds, so
it's important to see the linear model first. In this way. we can see how
to transport knowledge of the linear case to the nonlinear case of
manifolds. As it turns out, tensors are the proper way the think about
integration on a manifold, which is one of their many uses. \\
%
Recall that the \ib{dual space} $V^*$ of a vector space $V$ is the vector space
of linear maps $V \to \R$. Dual spaces will be the building blocks of
multilinear functions The central idea of tensors is \emph{multilinearity}.
Let $V_1 \ldots V_n$ be vector spaces. Then a function
$F : V_1 \times \cdots \times V_n \to \R$ is \ib{multilinear} if it is linear
in each term with the other terms fixed, i.e.
$$F(v_1, \ldots \lambda v_i + \mu v_i', \ldots, v_n) =
\lambda F(v_1, \ldots v_i, \ldots v_n) +
\mu F(v_1 \ldots, v_i', \ldots, v_n)$$
%
\begin{defn}
Let $V_1 \cdots V_k$ be finite dimensional vector spaces. Then the
\ib{tensor product} of $V_1 \cdots V_k$ is a vector space, denoted
$V_1 \otimes \cdots \otimes V_k$ equipped with a multilinear map
$V_1 \times \cdots \times V_k \to V \otimes \cdots \otimes V_k$ satisfying
the follwing \ib{universal property} : Given a multilinear map
$\varphi : V_1 \times \cdots \times V_k \to W$ for any vector space $W$,
there exists a unique linear map
$\tilde{\varphi} : V_1 \otimes \cdots \otimes V_k$ such that the following
diagram
$$\begin{tikzcd}
V_1 \times \cdots \times V_k \ar[dr, "\varphi"] \ar[d] \\
V_1 \otimes \cdots \otimes V_k \ar[r, "\tilde{\varphi}"'] & W
\end{tikzcd}$$
commutes.
\end{defn}
%
We've given the universal property, but we haven't guaranteed that such
a vector space exists.
%
\begin{thm}
Let $V_1, \ldots V_k$ be finite dimensional vector spaces, and let
$F(V_1, \ldots, V_k)$ denote the \ib{free vector space} on
$V_1, \ldots V_k$, i.e. formal linear combinations of $k$-tuples
$(v_1,\ldots,v_k)$ with $v_i \in V_i$. Then let $R$ denote the subspace
spanned by elements of the form
\begin{align*}
&(v_1, \ldots \lambda v_i, \ldots v_k) -
\lambda (v_1, \ldots v_i, \ldots v_k)\\
&(v_1, \ldots, v_i + v_i', \ldots v_k) - (v_1, \ldots v_i, \ldots v_k)
- (v_1, \ldots v_i', \ldots v_k)
\end{align*}
Let $v_1 \otimes \cdots \otimes v_k$ denote the image of $(v_1, \ldots v_k)$
under the quotient projection. Then the vector space $F(V_1, \ldots V_k) / R$,
equipped with the map $V_1 \times \cdots V_k \to F(V_1, \ldots V_k) / R$ given
by $(v_1, \ldots, v_k) \mapsto v_1 \otimes \cdots \otimes v_k$ satisfies the
universal property of the tensor product.
\end{thm}
%
The reason we are about tensor products, is that it gives us the tools to
describe multilinear maps.
%
\begin{thm}
Let $L(V_1, \ldots, V_k)$ denote the space of multilinear maps
$V_1 \times \cdots \times V_k \to \R$. Then there is a canonical isomorphism
$V_1^* \otimes \cdots \otimes V_k^* \cong L(V_1 \ldots, V_k)$
\end{thm}
%
\begin{proof}
We define $\varphi : V_1 \otimes \cdots \otimes V_k \to L(V_1, \ldots, V_k)$ by
specifying it's action on the spanning set of elements of the form
$v^1 \otimes \cdots \otimes v^k$ and extending linearly to linear combinations.
$$\varphi(v^1 \otimes \cdots \otimes v^k)(w_1, \ldots w_k) =
\prod_i v^i(w_i)$$
\end{proof}
%
\begin{exer}
Prove the map $\varphi$ is an isomorphism.
\end{exer}
%
\begin{defn}
A \ib{covariant} $k$-tensor on a vector space $V$ is an element of
$\underbrace{V^* \otimes \ldots \otimes V^*}_{k \text{ times}}$. A
\ib{contravariant} $k$ tensor is an element of $\underbrace{V \otimes \ldots \otimes V}_{k \text{ times}}$. A $(k,\ell)$-tensor is an element of
$\underbrace{V^* \otimes \ldots \otimes V^*}_{k \text{ times}} \otimes
\underbrace{V \otimes \ldots \otimes V}_{\ell \text{ times}}$. We denote the
vector space of covariant, contravariant, and mixed tensors as $T^k(V)$,
$T_k(V)$, and $T^k_\ell(V)$ respectively.
\end{defn}
%
For the most part, we will be focused on covaraint tensors, though
contravariant and mixed tensors do show up as well in differential geometry.
An important property of covariant tensors is that they \emph{pull back}
(which admittedly makes their name quite confusing -- the name is a relic
from older times). What we mean by that is that given vector spaces $V$ and
$W$, a linear map $\varphi : V \to W$ induces a map
$\varphi^* : T^k(W) \to T^k(V)$, where a $k$-tensor $\omega$ is mapped
to the tensor $\varphi^*\omega$ defined by
$$\varphi^*\omega(v_1, \ldots, v_k) = \omega(\varphi(v_1), \ldots \varphi(v_k))$$
%
%TODO do all the cotangent space stuff here, as well as tensor bundles/fields?
\section{Cotangent Spaces and Covectors}
%
Now that we've defined tensors on vector spaces, we want to transport this
concept fo the nonlinear case of manifolds.
\begin{defn}
Let $M$ be a smooth manifold, and $p \in M$. Then the \ib{cotangent space} at
$p$ is the dual space to $T_pM$, and is denoted $T^*_pM$. The
\ib{cotangent bundle} is the vector bundle $T^*M = \coprod_{p \in M}T_p^*M$.
\end{defn}
%
\begin{defn}
A \ib{covector field} is a global section of $T^*M$, i.e. for every $p \in M$,
we assign it a covector $\omega_p \in T_p^*M$. We denote the vector space of
covector fields as $\mathfrak{X}^*(M)$. Given a covector field $\omega$, we
often denote it's value at $p \in M$ as $\omega_p$.
\end{defn}
%
In particular, there's an easy way to obtain covector fields. Given a smooth
function $f \in C^\infty(M)$, define the \ib{differential} of $f$, denoted $df$,
to be the covector field $df_p(v) = vf$, where $c \in T_pM$. We note that this
is the same notation for the derivative, but in the case for a smooth
real-valued function, the two definitions essentially coincide. In coordinates,
we have that, from the definition, $df(\partial_i) = \partial_i f$, so
the components of $df$ in terms of the dual basis $\xi^i$ to the $\partial_i$
are the functions $\partial_i f$. In particular, we can compute the
differentials for the coordinate functions $x^i$. We have that
$$dx^j = \partial_i x^j \xi^j = \delta^j_i \xi^i = \xi^j $$
Therefore, the differentials $dx^i$ are the dual basis to the $\partial_i$.
We can then write the coordinate formula for $df$ in a more suggestive notation
$df = \partial_i f dx^i$, which looks like a gradient. \\

As with tangent vectors, we would like to know how the covectors $dx^i$
transform under a change of coordinates. Let $(y^1, \ldots y^n)$ denote another
set of coordinate functions, and let $\omega = \omega_i dx^i$ be some covector.
Then using the transformation law for the tangent vectors
$\partial/ \partial x^i$, we compute
$$\omega_i = \omega\left( \frac{\partial}{\partial x^i} \right)  =
\omega\left( \frac{\partial y^j}{\partial x^i}\frac{\partial}{\partial y^j} \right)
= \frac{\partial y^j}{\partial x^i}\tilde{\omega}^j$$
where $\tilde{\omega}^j$ denotes the component functions of $\omega$ with respect
to the basis $dy^i$. \\

As it turns out, covector fields are the natural things to integrate along
curves, and will form the basis for the discussion on integration on manifolds.
The main reason for this is that covector fields pull back, which means that
they play nicely with coordinate charts.
%
\begin{defn}
Let $F : M \to N$ be a smooth map of manifolds, and
$\omega \in \mathfrak{X}^*(N)$. Define the \ib{pullback} of $\omega$ along $F$
to be the covector field $F^*\omega \in \mathfrak{X}^*(M)$ defined by
$(F^*\omega)_p = dF_p^*\omega_p$, i.e. the point-wise pullback
$$dF_p^*\omega_p(v) = \omega_{F(p)}(dF_p(v)) $$
for $v \in T_pM$.
\end{defn}
%
Note that this is defined for \emph{any} smooth map $M \to N$, not just for
diffeomorphisms like in the case of vector fields.
%
\begin{exmp}[Computing pullbacks]
Computing pullbacks of covector fields when the domain and codomain are Euclidean
space is relatively easy, and agrees with the calculus idea of $u$-substitution.
For example, let $F : \R^2 \to \R^2$ where $F(s,t) = (st, e^t)$ and
$\omega = x dy - ydx$ (Exercise 11.8 in Lee). Then to compute the pullback $F^*\omega$,
we substitute $st$ for $x$ and $e^t$ for $y$, i.e.
\begin{align*}
F^*\omega &= st d(e^t) - e^td(st) \\
&= st(e^t dt) - e^t(tds + sdt) \\
&= (ste^t - se^t)dt - te^t ds
\end{align*}
\end{exmp}
%
\begin{defn}
Let $M$ be a smooth manifold. Then a \ib{smooth curve} is a smooth map
$\gamma : [0,1] \to M$.
\end{defn}
%
The key point here is that integration on the interval $[0,1]$ makes sense to
us, so given a covector field $\omega$, we want to define its integral in
terms of what we already know. Let $\omega$ be a covector field on $[0,1]$,
with global coordinate $t$. Then we know $\omega = f dt$ for some smooth
function $f$, and we can define the integral to be our usual notion of
integration
$$\int_{[0,1]} \omega = \int_0^1 fdt $$
%
\begin{defn}
Let $\gamma : [0,1] \to M$ be a smooth curve, and $\omega \in \mathfrak{X}^*(M)$.
Define the \ib{integral} of $\omega$ along $\gamma$ to be
$$\int_\gamma \omega = \int_{[0,1]}\gamma^*\omega $$
\end{defn}
%
\begin{exmp}[Exercise 11-14 in Lee]
Let $\omega$ be the covector field on $\R^3$ defined by
$$\omega = -\frac{4z}{(x^2 + 1)} dx + \frac{2y}{y^2 + 1}dy + \frac{2x}{^2 + 1} dz $$
and let $S$ denote the line segment from $(0,0,0)$ to $(1,1,1)$. To compute
the line integral $\int_S \omega$, we parameterize $S$ with $\gamma(t) = (t,t,t)$,
and compute the pullback to be
\begin{align*}
\gamma^*\omega = -\frac{4t}{(t^2 + 1)} dt + \frac{2t}{t^2 + 1}dt + \frac{2t}{^2 + 1} dt \\
= \left( \frac{4t}{t^2 + 1} - \frac{4t}{(t^2 + 1)^2} \right) dt
\end{align*}
which gives us that
$$\int_S \omega = \int_0^1 \left( \frac{4t}{t^2 + 1} - \frac{4t}{(t^2 + 1)^2} \right) dt$$
\end{exmp}
%
\begin{exer}
Define the covector field $\omega$ on $\R^2 - \set{0}$ by
$$\omega = \frac{x dy - ydx}{x^2 + y^2} $$
compute the integral of $\omega$ along the unit circle. (Note that the integral
along the unit circle is the same as the integral along the unit circle
missing a point, which can be parameterized with the curve
$\gamma(t) = (\cos t, \sin t))$
\end{exer}
%
\section{Differential Forms and the Exterior Derivative}
%
The central idea behind integration is \emph{volume}. If you think about your
calculus classes (especially vector calculus), you recall that the things
you integrate tend to be infinitesimal areas or volumes, and you might have
thought of the formal symbols $dA$ and $dx~dy$ as symbols representing
tiny squares or parallelogram, and you probably think of $dt$ as an
infinitesimal line segment. Therefore, it's in our best interest to
understand a general framework for volume. You've probablty encountered
the determinant of a matrix before, and you might have seen that it computes
the volume of the parallelopiped spanned by the columns of the matrix. This
will be the prototypical ``volume function" we will work with. There are
several properties of determinants that make them ideal ``volume functions,"
and there are some properties we should keep in mind.  \\

Firstly, the determinant is multilinear, much like how the volumes of
parelleopipieds change when we add or scale vectors. Secondly, the determinant
evaluates to $0$ on any linearly dependent set of vectors, which should be
intuitive, since we are missing a dimension for our parallelopiped. A
consequence of this is that the determinant is \emph{alternating} -- it
switches sign if we swap two columns of the matrix. This hints that we
should really be thinking about \emph{signed} volume. Therefore, the class
of volume functions we're looking for should be multiliear alternating
tensors on the vector space.
%
\begin{defn}
A tensor $\omega \in T^k(V)$ is \ib{alternating} if it changs sign whenever
two arguments are interchaged, i.e. for vectors $v_1, \ldots v_k$, we have that
for all distinct $i,j$,
$$\omega(v_1,\ldots, v_i, \ldots v_j, \ldots v_k) = -\omega(v_1, \ldots v_j, \ldots v_i, \ldots v_k) $$
Another way to define this is using the sign of a permutation. Given an permutation
$\sigma \in S_k$, the \ib{signature} of $\sigma$ is the number of transpositions
needed to generated $\sigma$, and is denoted $\sgn \sigma$. Then let $(-1)^\sigma$
denoted $(-1)^{\sgn\sigma}$. An equivalent defintion for $\omega$ to be alternating
is that
$$\omega(v_1, \ldots v_k) = (-1)^\sigma\omega(v_{\sigma(1)}, \ldots, v_{\sigma(k)}) $$
We denote the space of all alternating covariant $k$-tensors as $\Lambda^k(V^*)$.
\end{defn}
%
One wonderful fact about the alternating tensors is that the form an algebra under
a product called the \ib{wedge product}, and is denoted $\wedge$.
There's a few gory details to work out, which I will defer to Lee. We're mainly
concerened with their properties, which you can take as axioms.
If you want to read more into the construction of the wedge product,
chapter 14 in Lee should have everything you need.
%
We'll first lay out the rules for the wedge product.
\begin{enumerate}
  \item The wedge product $\wedge$ is bilinear, i.e.
  $$\omega \wedge (\eta + \xi) = \omega \wedge \eta + \omega \wedge \xi $$
  \item The wedge product is associative, i.e. $\omega \wedge (\eta \wedge \xi)
  = (\omega \wedge \eta) \wedge \xi)$.
  \item For $\omega \in \Lambda^k(V^*)$
  and $\eta \in \Lambda^\ell(V^*)$, we have that
  $\omega \wedge \eta \in \Lambda^{k + \ell}(V^*)$
  \item The wedge product is anticommutative, (sometimew called supercommutative
  if you are familiar with superalgebras) i.e. for $\omega \in \Lambda^k(V^*)$
  and $\eta \in \Lambda^\ell(V^*)$, we have that
  $\omega \wedge \eta = (-1)^{k\ell}\eta \wedge \omega$. In particular,
  note that for $k = \ell = 1$, this implies that $\omega \wedge \omega = 0$.
\end{enumerate}
%
Under this product, the vector space $\bigoplus_{k} \Lambda^k(V)$ forms an
algebra over $\R$, called the \ib{exterior algebra}. In particular, this algebra
is $\Z$-graded (if you know what that means).

The next thing we need to know is that given a $k$-form $\omega$ and
an $\ell$-form $\eta$, how does $\omega \wedge \eta$ act? We start in the simple
case. Let $\omega$ and $\eta$ both be $1$-forms, i.e. covectors. Then
$$(\omega \wedge \eta)(v_1,v_2) = \omega(v_1)\eta(v_2) - \omega(v_2)\eta(v_1)
= \det\begin{pmatrix}
  \omega(v_1) & \omega(v_2) \\
  \eta(v_1) & \eta(v_2)
\end{pmatrix}$$
In particular, given collection $\set{\omega^i}$ of covectors, their $k$-fold
wedge product acts on $k$-vectors by
$$\omega^1 \wedge \ldots \wedge \omega^k(v_1, \ldots ,v_k) = \det \omega^i(v_j)$$
If we fix a basis $\set{e_i}$ for $V$ and let $\set{\varepsilon^i}$ denote the
dual basis, then you can take for a fact that the set
$$\set{\varepsilon^{i_1} \wedge \ldots \wedge \varepsilon^{i_k}~:~
1 \leq i_1 < i_2 \ldots < i_k \leq \dim V} $$
is a basis for $\Lambda^k(V)$, which implies in particular that the dimension is
$\binom{\dim V}{k}$.
%
\begin{exer}
Show that if a set $\omega^1 \ldots \omega^k$ of covectors is linearly dependent,
then $\omega^1 \wedge \ldots \wedge \omega^k = 0$.
\end{exer}
%
We now move to the world of manifolds. First, we make a quick remark regarding
vector bundles. We have a lot of tools for constructing new vector spaces from
old (e.g. direct sum, tensor product, exterior algebra), and most of these
constructions carry over to vector bundles, where we do the linear algebraic
operations fiberwise. For example, for vector bundles $E \to M$ and $F \to M$,
we can form the \ib{Whitney sum} $E \oplus F$ by taking the fiberwise direct sum,
i.e. $(E \oplus F)_p = E_p \oplus F_p$. There's some care that the fiberwise
operations occur smoothly, which is made rigorous with the notion of a
\ib{smooth functor}. For more details, you can check out Exercise 10-8 in Lee.
For now, just assume that taking the exterior powers $\Lambda^k(V)$ forms
a smooth functor, so we can in particular do this to the cotangent bundle
of a manifold $M$, giving us vector bundles
$\Lambda^k(T^*M) = \coprod_{p \in M} \Lambda^k(T^*_pM)$ with the obvious
projections.
%
\begin{defn}
A section of $\Lambda^k(T^*M)$ is a \ib{differential} $k$\ib{-form}. The space
of $k$-forms is denoted $\Omega^k(M)$. The wedge product $\omega \wedge \eta$
for for $\omega \in \Omega^k(M)$ and $\eta \in \Omega^\ell(M)$ is the $k+\ell$
form where $(\omega \wedge \eta)_p = \omega_p \wedge \eta_p$. Note that for local
coordinates $(x^i)$, any $k$-form can be written locally in the form
$\omega = \sum_I \omega_I dx^{i_1} \wedge \ldots \wedge dx^{i_k}$, where the
sums is over all increasing sequences $I = (i_1, \ldots i_k)$ with
$1 \leq i_1 < \ldots < i_k \leq \dim M$. In the case that $\omega$ is a
$\dim M$-form, we call it a \ib{top degree form}. In addition, we let
$\Omega^0(M) = C^\infty(M)$.
\end{defn}
%
Like how covector fields ($1$-forms) were integrated over curves, $k$-forms
are objects meant to be integrated over $k$-dimensional submanifolds. Like
with $1-$forms, we mainly do this via pullback, so it's important to know
how to do the computations.
%
\begin{prop}
  Let $ F: M \to N$ be a smooth map.
  \begin{enumerate}
    \item $F^* : \Omega^k(N) \to \Omega^k(M)$ is linear
    \item $F^*$ commutes with the wedge product, i.e.
    $$F^*(\omega \wedge \eta) = F^*\omega \wedge F^*\eta $$
    \item Summing over all increasing sequences of numbers $1 \leq i_1 < \ldots i_k \leq \dim M$
    $$F^* \sum_I \omega_I dy^{i_1} \wedge \ldots \wedge dy^{i_k} =
    \sum_I (\omega_I \circ F) d(y^{i_1} \circ F) \wedge \ldots \wedge d(y^{i_k}\circ F)$$
  \end{enumerate}
\end{prop}
%
Using this, there's a simple way to express the pullback of a top degree form.
Let $F: M \to N$, for smooth $n$-manifolds $M$ and $N$.
with coordinates $(x^i)$ for $M$ and $(y^i)$ for $N$. Then
let $\omega = u dy^1 \wedge \cdots \wedge xy^n$ be a top degree form for $N$
for some smooth functions $u : N \to \R$. Then
$$F^*\omega = (u \circ F)(\det DF)dx^1 \wedge \ldots  dx^n$$
where $DF$ is the Jacobian of the coordinate representation of $F$.
This should look familiar, it's the change of coordinates formula in
multivariable calculus! This means that differential forms transform in
exactly the right manner for integration, as they encapsulate how parallelopipeds
transform by the determinant of the derivative.
%
%TODO Exterior derivative + Stokes Theorem + relationship with grad/curl
%Some integral computations?
%
There's an operation on differential forms called the \ib{exterior derivative},
which is a family of linear maps $\Omega^k(M) \to \Omega^{k+1}(M)$. By abuse of
notation we call all of them $d$. We've already seen one of these maps, namely
the differential $df$ of a function. We hope to generalize this to arbitrary
differential forms. This is best done in coordinates. \\

\begin{defn}
Let $\omega \in \Omega^k(M)$. Then in coordinates $(x^i)$, we can write
$$\omega = \sum_I \omega_I dx^{i_1} \wedge \cdots \wedge dx^{i_k}$$
summing over all increasing index sequences $I$. Then the \ib{exterior derivative}
of $\omega$, denoted $d\omega$ is the $(k+1)$ form given by
$$d\omega = \sum_I \frac{\partial \omega_I}{\partial x^i}dx^i \wedge
dx^{i_1} \wedge \cdots \wedge dx^{i_k} $$
\end{defn}
%
\begin{exer}
Let $x,y,z$ denote the standard coordinates on $\R^3$. Explicitly compute
the action $d$ on $\Omega^0(\R^3)$, $\Omega^1(\R^3)$, and $\Omega^3(\R^3)$.
In addition, try computing $d \circ d$. Does the exterior derivative
look like something you remember from vector calculus?
(Hint: You know how $d$ acts on $\Omega^0(\R^3)$. Then use the fact that
any $1$-form can be written globally as $f_1 dx + f_2dy + f_3 dz$, and
that any $2$-form can be written as $f_1 dx \wedge dy + f_2 dy \wedge dz +
f_3 dx \wedge dz$).
\end{exer}
%
A key observation to make regarding $d$ is that it is natural, in the sense
that it commutes with pullback, i.e. $F^*(d\omega) = d(F^*\omega)$. Another
observation is that $d \circ d = 0$, so (if you know the terminology)
this gives rise to a chain complex
$$\Omega^1(M) \xrightarrow{d} \Omega^{2}(M) \xrightarrow{d} \cdots
\Omega^{n-1}(M) \xrightarrow{d} \Omega^n(M) \to 0$$
called the \ib{de Rham Complex} of $M$. We say that a $k$-form $\omega$ is \ib{closed}
if $d\omega = 0$, and we say it is \ib{exact} if there exists a $(k-1)$ form $\eta$
such that $d\eta = \omega$.
%
\section{Integration on Manifolds}
%
As a quick remark, most of the discussion here relies on our manifld being
oriented. Essentially this involves a consistent assignment of ``handedness"
to our manifold. For now, just assume all the manifolds are oriented. \\

As per usual, we start in the Euclidean case, which is particularly nice. Let
$U \subset \R^n$ be open (there are less stringent conditions we can make on $U$,
but it's not necssary for our purposes). Then let $\omega$ be an $n$-form, which
can be written as $\omega = f dx^1 \wedge \cdots \wedge dx^n$ for some smooth
function $f$. Then define the integral of $\omega$ over $U$ as
$$\int_U \omega = \int_U f dx^1 \cdots dx^n $$
where the right hand integral is the one we are familar with. This amounts
to saying that integrating $\omega$ simply forgets all the wedge symbols. Recall
the change of variable formula for $\R^n$ involves the determinant of the Jacobian
and that the pullback formula for a $n$-form $\omega$ has this built in. This
tells us that this integral is defined in a coordinate independent way. Namely,
if we had a diffeomorphism $F : U \to V$, and an $n$-form $\omega$ defined on $V$,
then $\int_V \omega = \int_U F^*\omega$ (assuming the diffeomorphism
is orientation preserving). \\

We now want to transfer this notion of integration to an abitrary manifold $M$.
The first observation is that if an $n$-form $\omega$ is supported in some coordinate chart
$(U,\varphi)$, which means that $\omega = 0$ outside of $U$, then this is easy,
since we can define $\int_M \omega = \int_{\varphi(U)} (\varphi\inv)^*\omega$.
In the general case, you need something called a \ib{partition of unity} subordinate
to an open cover of $M$ by coordinate charts, but we will not dive into the details. \\

The big theorem here involves integration on manifolds with boundary. It is a
vast generalization of the fundamental theorem of calculus, and also encapsulates
some of the key theorems in vector calculus, like Green's Theorem and the Divergence
Theorem.
%
\begin{thm} [\ib{Stokes' Theorem}]
Let $M$ be a smooth $n$-manifold with boundary, and $\omega \in \Omega^{n-1}(M)$.
Then
$$\int_{\bd M} \omega = \int_M d\omega$$
\end{thm}
%
This is an extremely important computational tool. In particular, note that if $M$
is a manifold without boundary, then the integral of any exact form $\omega = d\eta$
over $M$ is $0$, since
$$\int_M \omega = \int_\emptyset \eta = 0 $$
%
\begin{exer}
Consider the torus $\mathbb{T} \cong S^1 \times S^1 \subset \R^4$, where
$$\mathbb{T} = \set{(w,x,y,z) ~:~ w^2 + x^2 = 1, ~ y^2 + z^2 = 1} $$
Compute the integral of $\omega = xyz dw \wedge dy$ over $\mathbb{T}$.(Hint:
parameterize $\mathbb{T}$ with the map $\gamma \times \gamma$, where
$\gamma : [0,2\pi) \to S^1$ is the map $\theta \mapsto (\cos\theta,\sin\theta)$
this misses a circle on the torus, but it is measure $0$, so you can ignore it.)
\end{exer}
%
