\documentclass[psamsfonts]{amsart}
%
%-------Packages---------
%
\usepackage[h margin=1 in, v margin=1 in]{geometry}
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{tikz-cd}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{mathpazo}
%\usepackage{eulervm}
\usepackage{yfonts}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{fourier-orns}
\usepackage[all]{xy}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{url}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{pdfsync}
\usepackage{mathdots}
\usepackage{calligra}
%
\usepackage{tgpagella}
\usepackage[T1]{fontenc}
%
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Matlab,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
  }
%
%--------Theorem Environments--------
%
\newtheorem{thm}{Theorem}[section]
\newtheorem*{thm*}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem*{quest}{Question}
%
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{defn*}{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}
\newtheorem{TODO}{\ib{TODO}}
%
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem*{claim}{Claim}
\newtheorem*{aside*}{Aside}
\newtheorem*{rem*}{Remark}
\newtheorem*{hint*}{Hint}
\newtheorem*{note}{Note}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}
%
%--------Macros--------
\renewcommand{\qedsymbol}{$\blacksquare$}
\renewcommand{\hom}{\mathsf{Hom}}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\O}{\mathscr{O}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\ib}[1]{\textbf{\textit{#1}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\M}{\mathcal{M}}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\V}{\vec{v}}
\newcommand{\RP}{\mathbb{RP}}
\newcommand{\CP}{\mathbb{CP}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\GL}{\mathsf{GL}}
\newcommand{\SL}{\mathsf{SL}}
\newcommand{\SP}{\mathsf{SP}}
\newcommand{\SO}{\mathsf{SO}}
\newcommand{\SU}{\mathsf{SU}}
\newcommand{\gl}{\mathfrak{gl}}
\newcommand{\g}{\mathfrak{g}}
\newcommand{\inv}{^{-1}}
\newcommand{\bra}[2]{ \left[ #1, #2 \right] }
\newcommand{\ind}{\lambda \in \Lambda}
\newcommand{\set}[1]{\left\lbrace #1 \right\rbrace}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\transv}{\mathrel{\text{\tpitchfork}}}
\newcommand{\enumbreak}{\ \\ \vspace{-\baselineskip}}
\let\oldexists\exists
\renewcommand\exists{\oldexists~}
\let\oldL\L
\renewcommand\L{\mathfrak{L}}
\makeatletter
\newcommand{\tpitchfork}{%
  \vbox{
    \baselineskip\z@skip
    \lineskip-.52ex
    \lineskiplimit\maxdimen
    \m@th
    \ialign{##\crcr\hidewidth\smash{$-$}\hidewidth\crcr$\pitchfork$\crcr}
  }%
}
\makeatother
\newcommand{\bd}{\partial}
\newcommand{\lang}{\begin{picture}(5,7)
\put(1.1,2.5){\rotatebox{45}{\line(1,0){6.0}}}
\put(1.1,2.5){\rotatebox{315}{\line(1,0){6.0}}}
\end{picture}}
\newcommand{\rang}{\begin{picture}(5,7)
\put(.1,2.5){\rotatebox{135}{\line(1,0){6.0}}}
\put(.1,2.5){\rotatebox{225}{\line(1,0){6.0}}}
\end{picture}}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\grap}{graph}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\inter}{Int}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\indx}{ind}
\DeclareMathOperator{\alt}{Alt}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Cliff}{Cliff}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\dv}{div}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\Pin}{Pin}
\DeclareMathOperator{\Spin}{Spin}
\DeclareMathOperator{\sheafhom}{\mathscr{H}\text{\kern -3pt {\calligra\large om}}\,}
\newcommand*\myhrulefill{%
   \leavevmode\leaders\hrule depth-2pt height 2.4pt\hfill\kern0pt}
\newcommand\niceending[1]{%
  \begin{center}%
    \LARGE \myhrulefill \hspace{0.2cm} #1 \hspace{0.2cm} \myhrulefill%
  \end{center}}
\newcommand*\sectionend{\niceending{\decofourleft\decofourright}}
\newcommand*\subsectionend{\niceending{\decosix}}
\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}
%
%--------Hypersetup--------
%
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=black
}
%
%--------Solution--------
%
\newenvironment{solution}
  {\begin{proof}[Solution]}
  {\end{proof}}
%
%--------Graphics--------
%
%\graphicspath{ {images/} }

\begin{document}
\author{Jeffrey Jiang}
\title{Spin Geometry Conference Course}
\maketitle
%
\setcounter{section}{1}
\section*{Week 1}
%
\begin{exer}
Prove $SL_n(\R)$ and $O(n)$ are manifolds
\end{exer}
%
\begin{exer}
What is the ``shape" of $SL_2(\R)$?
\end{exer}
%
\begin{exer}
Prove that
$$O(2) = \set{\begin{pmatrix}
\cos\theta -\sin\theta \\
\sin\theta \cos\theta
\end{pmatrix}} \cup \set{\begin{pmatrix}
\cos\theta \sin\theta \\
\sin\theta -\cos\theta 
\end{pmatrix}}$$
The first set consists of rotations and the second set consists of reflections. Which rotations commute? Which reflections commute? Do reflections commute with reflections?
\end{exer}
%
\begin{exer}
Investigate $O(3)$. What is it's ``shape?"
\end{exer}
%
\setcounter{section}{2}
%
\setcounter{thm}{0}
%
\section*{Week 2}
\begin{exer}
What is the derivative of $\det$?
\end{exer}
%
\begin{exer}
Explore the exponetial map $\mathfrak{sl}_2(\R) \to SL_2(\R)$
\end{exer}
%
\begin{exer}
Prove that every element of $O(n)$ can be written as the composition of at most $n$ reflections about hyperplanes in $\R^n$.
\end{exer}
%
\begin{proof}
We do this by induction. For $n = 1$, this is obvious, since $O(1) \cong \pm 1$. The assuming that this holds for dimension $n-1$, Let $A \in O(n)$, and let $v \in \R$. We want to construct a hyperplane reflection $R$ such that $RAv = v$, which is obtained by taking $R$ to be the hyperplane reflection about the bisector of $v$ and $Av$. More explicitly, take $R$ to be the hyperplane reflection about the vector 
$$\frac{Av - v}{\norm{Av - v}} $$
which is given by the equation
$$Rw = w - 2 \frac{\langle Av - v, v \rangle}{\langle Av - v, Av - v \rangle}(Av - v) $$
Computing its action on $v$, we get 
\begin{align*}
Rv &= v - 2 \frac{\langle Av - v, v \rangle}{\langle Av - v, Av - v \rangle}(Av - v) \\
&= v - \frac{2\langle Av, v \rangle - 2\langle v, v \rangle}{2 \langle v, v \rangle - 2 \langle Av, v \rangle}(Av - v) \\
&= v + Av -v \\
&= Av
\end{align*}
Then since $R$ is its own inverse (being a reflection), we have that $RAv = v$, so $RAv$ fixes $v$ and its orthogonal complement.
\end{proof}
%
\begin{TODO}
Add motivation for $A^\pm_n$
\end{TODO}
%
\begin{defn}
Define $A^\pm_n$ to be the unital algebra generated by $\R^n$ such that $\xi^2 = \pm 1$. and $\xi\eta = ?~\eta\xi$. Determine the sign of $\eta\xi$. Explore these algebras. Find $A\pm_1$, $A^\pm_2 \ldots$. What are they isomorphic to? Can you identify $O(n)$ as a subgroup?
\end{defn}
%
\setcounter{section}{3}
%
\setcounter{thm}{0}
%
\section*{Week 3}
\begin{exer}
Classify the algebras $A^+_n$ (we messed these up week 2).
\end{exer}
%
\begin{exer}
Prove that 
$$\set{e_{i_1}e_{i_2}\ldots e_{i+k} ~|~ 1 \leq i_1 < i_2 < \ldots < i_k \leq n} $$
is a basis for $A^\pm_n$.
\end{exer}
%
\begin{exer}
Modify the isomorphisms found for $A^-_n$ by choosing $\Z/2\Z$ gradings for the domains and codomains such that the isomorphisms are now isomorphisms as superalgebras.
\end{exer}
%
\begin{exer}
Construct a tensor product for super vector spaces and superalgebras.
\end{exer}
%
\begin{exer}
Explore the ``shape" of the group 
$$G = \langle v ~|~ \norm{v} = 1 \rangle \subset (A_n^-)^\times $$
and the nature of the surjection $G \twoheadrightarrow O(n)$. What is the kernel of this map?
\end{exer}
%
\setcounter{section}{4}
%
\setcounter{thm}{0}
%
\section*{Week 4}
%
\begin{exer}
Define $\varphi : A^\pm_n \to A^\pm_n$ by $\varphi(v) = -v$ and $\varphi(vw) = (wv)$ (i.e. $\varphi$ reverses products), and extending linearly to sums. Does $\varphi(x)\cdot x$ define a norm on $A^\pm_n$?
\end{exer}
%
\begin{exer}
Let $(A, \norm{})$ be a normed $\R$-algebra such that $\norm{ab} \leq \norm{a}\norm{b}$ for all $a,b \in A$. Show that the multiplicative units form an open subset.
\end{exer}
We note that an algebra element $a \in A$ determines a linear map $L_a : A \to A$ by left multiplication, i.e. $L_a(b) = ab$. By fixing a basis for $A$ as a vector space, we get an assignment $a \mapsto M_a$, where $M_a$ is the matrix for $L_a$ in this basis. We claim that an element $a \in A$ is a unit iff $\det M_a \neq 0$. To see this, we note that if $L_a$ is not invertible, then $a$ certainly cannot be, since otherwise $L_{a\inv}$ would be an inverse. For the other direction, we note that if $a$ is not a unit, then $L_a$ is not surjective, since $1_A$ is not in the image. We then claim that this mapping $a \mapsto M_a$ is continuous. Do do this, define a norm on the space of linear maps on $A$ by
$$\norm{M} = \sup_{v \in A} \frac{\norm{Mv}}{\norm{v}} $$
Then given $a,b \in A$, we compute
\begin{align*}
\norm{M_{a-b}} &= \sup_{v \in A} \frac{\norm{(a-b)v}}{\norm{v}} \\
&\leq \sup_{v \in A} \frac{\norm{a-b}\norm{v}}{\norm{v}} \\
&\leq \norm{a-b}
\end{align*}
So as $b \to a$, we have that $\norm{M_{a-b}} \to 0$ as well, so this mapping is continuous. Therefore, the mapping $a \mapsto \det M_a$ is then continuous, which makes the group of units $A^\times$ an open set, being the preimage of the open set $\R - \set{0}$.

One thing to note is that the argument we use to show that $a \mapsto M_a$ is continuous works with any norm such that $\norm{ab} \leq c \norm{a}\norm{b}$ for any constant $c$. Therefore, we have a small lemma regarding finite dimensional algebras with an inner product.
%
\begin{lem*}
Let $A$ be an $n$-dimensional algebra with inner product $\langle \cdot,\cdot\rangle$, and let $\norm{\cdot}$ denote the norm induced by the inner product $\norm{x}^2 = \langle x, x \rangle$. Then for all $xy \in A$, we have 
$$\norm{xy} \leq n^5\abs{\Gamma}\norm{a}\norm{b} $$
where $\Gamma$ denotes the structure constant of maximal magnitude with respect to a fixed orthonormal basis.
\end{lem*}
%
\begin{proof}
Fix a basis $\set{e_i}$ for $A$, and let $c^k_{ij}$ denote the structure constants where
$$e_ie_j = c^k_{ij}e_k $$
Then let $x = a^ie_i$ and $y = b^je_j$. We then compute
\begin{align*}
\norm{xy}^2 &= \langle a^ib^je_ie_j, a^\ell b^m e_\ell e_m \rangle \\
&= \langle a^ib^jc^k_{ij}e_k, a^\ell b^m c^n_{\ell m}e_n \rangle \\
&= a^i a^\ell b^j b^m c^k_{ij} c^n_{\ell m} \langle e_k , e_n \rangle \\
&\leq a^i a^\ell b^j b^m \Gamma^2  n \\
&\leq n^{5/2} \Gamma^2 \norm{a}\norm{b}
\end{align*}
\end{proof}
Because of this, we have that for the Clifford algebras $A^\pm_n$, the mapping from algebra elements to linear maps on the algebra is continuous, regardless of our choice of inner product. We can then use this to define a nicer norm on the Clifford algebras. First fix an arbitrary inner product and denote the induced norm $\norm{\cdot}_1$. Then define 
$$\norm{a} = \sup_{v \in A^{\pm}_n}\frac{\norm{av}_1}{\norm{v}_1} $$
which gives us a submultiplicative norm, so the group of units is an open subset. 

We now want to prove that $G$ is a topological group. To do this, it suffices to show that multiplication and inversion are continuous on $A^\pm_n$. For multiplication, fix $c,d \in A$, and suppose we have $a,b \in A$ such that
$$\norm{a - c} < \varepsilon \qquad \norm{b -d } < \varepsilon $$
for small $\varepsilon > 0$. Then we have 
\begin{align*}
\norm{ab - cd} &= \norm{ab - ad + ad - cd} \\
&= \norm{a(b-d) + (a-c)d} \\
&\leq \norm{a(b-d)} + \norm{(a-c)d} \\
&\leq \norm{a}\norm{b-d} + \norm{a-c}\norm{d} \\
&\leq (\norm{a} + \norm{d})\varepsilon \\
& \leq (\norm{a - c + c} + \norm{d})\varepsilon \\
&\leq (\norm{a - c} + \norm{c} + \norm{d})\varepsilon \\
&\leq (\varepsilon + \norm{c} + \norm{d})\varepsilon
\end{align*}
so multiplication is continuous at $(c,d)$. 
%
\begin{TODO}
Show inversion is continuous
\end{TODO}
%
\begin{exer}
An algebra $A$ is called a \ib{matrix algebra} if there exists an isomorphism $A \cong \End(V)$ for some vector space $V$. Which $A^\pm_n$ are matrix algebras?
\end{exer}
We explore what it means for $A^\pm_n$ to be a matrix algebra. We have that the isomorphism $\varphi: A^\pm_n \to \End(V)$ induces a Clifford module structure on $V$, where the action on $V$ is exactly $a \cdot v = \varphi(a)v$. What does $\varphi$ being an isomorphism imply about the module structure on $V$? We note that there cannot exist any invariant subspaces of $V$ under that action, since there always exists an endomorphism of $V$ that moves a subspace off of itself. Therefore, there cannot exist any $A^\pm_n$-submodules of $V$. Therefore, for $A^\pm_n$ to be a matrix algebra, there certainly must exist an irreducible $A^\pm_n$-module $V$.

From the universal property we laid out below, we have that out map $A^\pm_n \to \End(V)$ is equivalent data to a map $j: (V, b) \to \End(V)$ satisfying the relation $j(v)^2 = \pm b(v,v)\id_V$
%
\begin{exer}
Given a unital associative algebra $A$ and left $A$-modules $M$ and $N$, how would you form the direct sum? Can you tensor them? What if $A$ was a super algebra and $M,N$ super vector spaces?
\end{exer}
%
\begin{exer}
Let $V$ be a vector space and $b : V \times V \to V$ a bilinear form. We want to construct the Clifford algebra $\Cliff(V,b)$ as the ``best" associative unital $\R$-algebra generated by $V$ subject to the relation 
$$v_1v_2 + v_2v_1 = 2b(v_1,b_2)1_A$$
where $1_A$ denotes the multiplicative unit in $A$.
\end{exer}
We claim that the above relation is equivalent to the relation $v^2 = b(v,v)1_A$. To see this, we first note that the above condition implies this when we take $v_1 = v_2$. Then for the other direction, consider
$$(v_1 + v_2)^2 = v_1v_2 + v_2v_1 + v_1^2 + v_2^2$$
We then apply our relation, giving us
\begin{align*}
&b(v_1 + v_2, v_1 + v_2) = v_1v_2 + v_2v_1 + b(v_1,v_1) + b(v_2,v_2) \\
\implies & b(v_1 + v_2, v_1 + v_2) - b(v_1, v_1) - b(v_2, v_2) = v_1v_2 + v_2v_1
\end{align*}
Then applying polarization, we arrive at the desired identity.

With this, we want to construct $\Cliff(V,b)$ as the unital algebra satisfying our relation and subject to no others (other than bilinearity of multiplication). Therefore, we can consider the quotient of the tensor algebra $\mathcal{T}(V)$ by the ideal $(v^2 - b(v,v))$ to construct $\Cliff(V,b)$. To characterize it, we think of it as the universal such algebra containing $V$ subject to our relation. Since it is subject to no other relations, we expect this object to be \emph{initial}. It should have a map into every other such algebra satisfying this relation. In other words, for every algebra $A$ with an inclusion $j : V \hookrightarrow A$ such that $j(v_1)j(v_2) + j(v_2)j(v_1) = 2b(v_1,v_2)1_A$, we get a unique map $\Cliff(V,b) \to A$ such that the following diagram commutes
$$\begin{tikzcd}
V \arrow[hookrightarrow]{d} \arrow[hookrightarrow]{dr}{ j} \\
\Cliff(V,b) \arrow[dashed]{r} & A
\end{tikzcd}$$
In other words, the data of a map $\Cliff(V,b) \to A$ is equivalent to a map $j : V \to A$ satisfying the relation we want.

We claim that this characterizes the Clifford algebra up to unique isomorphism. Let $A$ be another algebra with map $j: V\hookrightarrow A$ satisfying the same property we gave above. From the universal property of $\Cliff(V,b)$, we get a unique map $\Cliff(V,b) \to A$. Likewise, the inclusion $V \hookrightarrow \Cliff(V,b)$ gives us a unique map $A \to \Cliff(V,b)$. We claim that these two maps are inverses. We note that both maps are given by $v \mapsto j(v)$ $j(v) \mapsto v$ and extending to products and sums, so we have that they are inverses.

We now want to verify that the construction $\mathcal{T}(V) / (v^2 - b(v,v))$ satisfies this universal property, i.e. the Clifford algebra exists. Given an algebra $A$ with a map $j : V \hookrightarrow A$ satisfying $j(v)^2 - b(v,v) = 0$, define the map $\mathcal{T}(V) \to A$ by $v \mapsto j(v)$ and extending linearly and to products. Then the ideal $(v^2 - b(v,v))$ lies in the kernel of this map, so the map factors through uniquely through $\mathcal{T}(V) / (v^2 - b(v,v))$, so it satisfies the property we laid out.
%
\setcounter{section}{5}
%
\setcounter{thm}{0}
%
\section*{Week 5}
%
\begin{exer}
Continue thinking about what makes an algebra a matrix algebra, either for a regular vector space, or for a super vector space.
\end{exer}
%
\begin{exer}
Is $\mathbb{H}$ isomorphic to an endomorphism algebra $\End(V)$ for some $V$? Does $\mathbb{H}$ admit an irreducible module?
\end{exer}
The quaternions $\mathbb{H}$ are not an endomorphism algebra because they are a division algebra -- there's too many invertible elements! It does admit an irreducible module however, namely itself. To show that it is irreducible, we show that it cannot have any proper invariant subspaces. To show this, we note that for any $q,p \in \mathbb{H}$, we have an element that when multiplied on the left with $q$ gives us $p$, namely $pq\inv$.
%
\begin{exer}
For an algebra $A$, an $A$-module $M$ is indecomposable if it can be expressed as the direct sum
$M = M_1 \oplus M_2$ of submodules $M_1$ and $M_2$. Any algebra acts on itself via left multiplication, which we will call the left regular representation. Is the left regular representation of a Clifford algebra indecomposable?
\end{exer}
%
\begin{quest}
How many irreducible representations are there for any given Clifford algebra $\Cliff(V,b)$. How many indecomposable ones?
\end{quest}
%
\begin{quest}
Does Schur's lemma hold for Clifford modules? Does Maschke's theorem hold?
\end{quest}
%
Clearly, an irreducible module is indecomposable, so this raises the question -- is the left regular representation for a Clifford algebra irreducible?
%
\begin{exer}
Given an $A$-module $M$ and a $B$-modules $N$, how would you realize $M \otimes N$ as a $A \otimes B$ module? What if $A$ and $B$ were superalgebras?
\end{exer}
We have that a module $V$ over an algebra $R$ is equivalent to giving an algebra homomorphism $R \to \End(V)$. Therefore, we have algebra homomorphisms $\varphi : A \to \End(M)$ and $\psi : B \to \End(N)$. Then we can use these maps to define a map $A \times B \to \End(M) \otimes \End(N)$ where $(a,b) \mapsto \varphi(a) \otimes \psi(v)$, which descends to the tensor product. Composing with the canonical isomorphism $\End(M) \otimes \End(N) \to \End(M \otimes N)$ then gives us the $A \otimes B$-module structure we desire on $M \otimes N$.

More explicitly, this more or less does what you expect, where the action of an element $a \otimes b$ is given by
$$(a\otimes b) \cdot (m \otimes n) = am \otimes bn $$
and extending linearly to sums of algebra elements.
In the case of superalgebras, the super tensor product and super vector spaces, this amounts to the same thing as we gave with the isomorphism $\End(V) \otimes \End(W) \to \End(V \otimes W)$, where the first tensor product is now the tensor product of superalgebras, this amounts to the module structure being
$$(a \otimes b) \cdot (m \otimes n) = (-1)^{|b||m|}(am \otimes bn) $$
%
\begin{exer}
Using only the universal property of the Clifford algebra,
\begin{enumerate}
\item Show the map $\iota : V \to \Cliff(V,b)$ is injective
\item Carefully state what it means for the $\Cliff(V,b)$ to be unique, and prove it
\item Can you obtain the $\Z/2\Z$ grading?
\item How do you get maps between Clifford algebras?
\end{enumerate}
\end{exer}
We first state a refined version of the universal property of $\Cliff(V,b)$. For a vector space $V$ with symmetric bilinear form $b : V \times V \to \R$, the \ib{Clifford algebra} is the data of a unital associative algebra $\Cliff(V,b)$ and a map $\iota : V \to \Cliff(V,b)$ such that for any algebra $A$ with a linear map $j : V \to A$ satisfying $j(v)^2 = b(v,v)$, we get a unique algebra homomorphism $\Cliff(V,b) \to A$ such that the following diagram commutes 
$$\begin{tikzcd}
V \arrow[d, "\iota"'] \arrow[dr, "j"]\\
\Cliff(V,b) \arrow[r] & A
\end{tikzcd}$$
\begin{enumerate}
\item Consider the map $j : V \to \mathcal{T}(V) / (v^2 - b(v,v))$ induced by the inclusion map $V \hookrightarrow \mathcal{T}(V)$.  We note that this map is injective, and satisfies the relation we need to get a map $\varphi : \Cliff(V,b) \to \mathcal{T}(V) / (v^2 - b(v,v)$ such that $j = \varphi \circ \iota$. Therefore, $\iota$ must be injective.
\item For uniqueness, suppose $(A, j)$ is another algebra satisfying the universal property of the Clifford algebra. Then we claim that there is a unique isomorphism $\Cliff(V,b) \to A$ such that 
$$\begin{tikzcd}
V \arrow[d, "\iota"'] \arrow[dr, "j"]\\
\Cliff(V,b) \arrow[r] & A
\end{tikzcd}$$
From the universal property of $\Cliff(V,b)$, we get a unique map $\varphi : \Cliff(V,b) \to A$, and since $A$ satisfies the universal property, the map $\iota$ gives us a unique map $\psi : A \to \Cliff(V,b)$. We claim that these maps are inverses (and consequently, isomorphisms). We note that $\psi \circ \varphi : \Cliff(V,b) \to \Cliff(V,b)$ satisfies $(\psi \circ \varphi)(\iota(v)) = \iota(v)$. so it must be the unique map that makes
$$\begin{tikzcd}
V \arrow[d, "\iota"'] \arrow[dr, "\iota"]\\
\Cliff(V,b) \arrow[r] & \Cliff(V,b)
\end{tikzcd}$$
commute. We note that the identity map on $\Cliff(V,b)$ satisfies this property, so by uniqueness, we must have $\psi \circ \varphi = \id$. Repeating the argument with $j$ and $A$, we conclude that $\varphi \circ psi = \id$, so they are inverses.
\item To obtain the grading, we want to show that the assignment $(V,b) \to \Cliff(V,b)$ is functorial. Given vector spaces $V,W$ with symmetric bilinear forms $b_V, b_W$ respectively, and a linear map $T : V \to W$ satisfying $b_W(Tv_1, Tv_2) = b_V(v_1, v_2)$ (i.e. $T^*b_W = b_V$). Then we claim we get an induced algebra homomorphism $T_* : \Cliff(V,b_V) \to \Cliff(W, b_W)$  such that
$$\begin{tikzcd}
(V, b_V) \arrow[r, "T"] \arrow[d, "\iota_V"']& (W, b_W) \arrow[d, "\iota_W"] \\
\Cliff(V, b_V) \arrow[r, "T_*"'] & \Cliff(W, b_W)
\end{tikzcd}$$
commutes. We note that the data of a map $\Cliff(V,b_V) \to \Cliff(W, b_W)$ is equivalent to the data of a linear map $j : V \to \Cliff(W, b_W)$ satisfying $j(v)^2 = b_V(v,v)$. In this case, let $j = \iota_W \circ T$. We note that $(\iota_W \circ T) (v)^2 = b_W(Tv, Tv)$. Then since we have that $T^*b_W = b_V$, we have that this is equal to $b_V(v,v)$, so we do indeed get an induced map $T_*: \Cliff(V,b_V) \to \Cliff(W, b_W)$, which is uniquely defined by the rule
$$T_* (\iota_V(v)) = \iota_W(Tv) $$
We claim that this is functorial, i.e. given $T: (V, b_V) \to (W,b_W)$ and $L : (W, b_W) \to (X, b_X)$, we have that $(L \circ T)_* = L_* \circ T_*$. This follows from looking at the diagram
$$\begin{tikzcd} 
V \arrow[d, "\iota_V"']\arrow[r, "T"] & W \arrow[d, "\iota_W"] \arrow[r, "L"] & X \arrow[d, "\iota_X"]\\
\Cliff(V, b_V) \arrow[r, "T_*"'] & \Cliff(W, b_W) \arrow[r, "L_*"'] & \Cliff(X, b_x)
\end{tikzcd}$$
With that out of the way, we can use the functoriality to obtain the $\Z/2\Z$ grading. Let $(V,b)$ be a vector space $V$ with symmetric bilinear form $b$, and consider the map $-\id_V$, where $v \mapsto -v$. Then we note that 
$$(-\id_V^*b)(v,w) = b(-v,-w) = b(v,w)$$
 so it induces an algebra map $\varphi : \Cliff(V,b) \to \Cliff(V,b)$ which is defined by the property that $\varphi(\iota(v)) = -\iota(v)$. Then let the even subspace of $\Cliff(V,b)$ be the subspace spanned by elements $a$ such that $\varphi(a) = a$, and let the odd subspace of $\Cliff(V,b)$ be the one spanned by elements $a$ such that $\varphi(a) = -a$.
 \item This is answered by the functoriality of $(V,b) \mapsto \Cliff(V,b)$. We get maps between Clifford algebras when we have linear maps that pullback the bilinear forms.
\end{enumerate}
%
\begin{exer}
Give a canonical isomorphism $\End(V) \otimes \End(W) \to \End(V \otimes W)$ in the case of regular vector spaces and super vector spaces.
\end{exer}
Let $A \in \End(V)$ and $B \in \End(W)$. Then define the map 
\begin{align*}
A \otimes B : V \otimes W &\to V \otimes W \\
v \otimes w &\mapsto Av \otimes Bw
\end{align*}
This defines a bilinear mapping $\End(V) \times \End(W) \to \End(V \otimes W)$, so it factors uniquely to map $\varphi : \End(V) \otimes \End(W) \to \End(V \otimes W)$. We claim that this defines an isomorphism. To show this, we note that by dimension, it suffices to show that the map is surjective. To show this, fix bases $\set{v_i}$ and $\set{w_j}$ for $V$ and $W$ respectively. Then $\set{v_i \otimes w_j}$ is a basis for $V \otimes W$. If we can find maps $A,B$ such that $A \otimes B$ maps $v_i \otimes v_j$ to $v_k \otimes v_\ell$ and maps the rest of the basis to $0$, we claim that this implies surjectivity. To see this, we note that any linear map $T \in \End(V \otimes W)$ is defined by its action on $\set{v_i \otimes w_j}$, which it maps to linear combinations of $\set{v_i \otimes w_j}$ therefore, we can construct any linear map we desire out of linear combinations of these elementary linear maps. To construct these elementary linear maps. Then fix $v_i \otimes w_j$ and $v_k \otimes w_\ell$. Then we know there exists a linear map $A \in \End(V)$ that maps $v_i$ to $v_k$ and the rest of the basis to $0$. In addition, we know that there exists a linear map $B \in \End(W)$ that maps $w_j$ to $w_\ell$ and the rest to $0$. Then $A \otimes B$ is the map we desire. Consequently, the map is surjective, and is an isomorphism by dimension count.

In the case that $V$ and $W$ are super vector spaces, write
\begin{align*}
V &= V^0 \oplus V^1 \\
W &= W^0 \oplus W^1
\end{align*}
We then note that the grading for $V$ and $W$ gives natural gradings for $\End(V)$ and $\End(W)$, where the even subspaces are the ones that preserve the grading and the odd subspaces are the ones that reverse the grading. In other words, a map $T \in \End(V)$ is even if $T(V^i) \subset V^i$ and is odd if $T(V^i)  = T(V^{i+1})$, where the addition is done mod $2$. In addition, we have that the gradings of $V$ and $W$ induce a grading on $V \otimes W$ where
\begin{align*}
(V \otimes W)^0 &= (V^0 \otimes W^0) \oplus (V^1 \otimes W^1) \\
(V \otimes W)^1 &= (V^0 \otimes W^1) \oplus (V^1 \otimes W^0)
\end{align*}
Therefore, in the case of $V$ and $W$ being super vector spaces, we want the isomorphism we construct 
$$\End(V) \otimes \End(W) \to \End(V \otimes W)$$
to respect this extra structure, which it does.

However, when we are the super tensor product $\End(V) \otimes \End(W)$ (where we use $\otimes$ to denote the super tensor product rather than the ordinary one), we have to be more careful, since the multiplication is now given by
$$(A \otimes B)(C \otimes D) = (-1)^{|B||C|}(AB \otimes CD) $$
and our original isomorphism no longer works. Define the action of a homoegenous element $A \otimes B$ on $v \otimes w$ by
$$(A \otimes B)(v \otimes w) = (-1)^{|B| |v|}(Av \otimes Bw) $$
we claim that this gives us an algebra map $\End(V) \otimes \End(W) \to \End(V \otimes W)$. To show this, we need to show that 
$$(A \otimes B)(C \otimes D)(v \otimes w) = (-1)^{|B||C|}(AC \otimes BD)(v \otimes W) $$
For the left hand side, unrolling the action we defined givues us
$$(A \otimes B)(C \otimes D)(v \otimes w) = (-1)^{|B||Cv| + |D||v|}(ACv \otimes BDw)$$
We note that $|Cv| = |C| + |v|$ mod $2$, so this gives us
$$ (A \otimes B)(C \otimes D)(v \otimes w) = (-1)^{|B||C| + (|B| + |D|)|v|}(ACv \otimes BDw)$$ 
On the right hand side, we have that 
$$(-1)^{|B||C|}(AC \otimes BD)(v \otimes w) =  (-1)^{|B||C| + |BD||v|}(ACv \otimes BDw)$$
We note that $|BD| = |B| + |D|$ by the definition of a super algebra, so this becomes
$$(-1)^{|B||C|}(AC \otimes BD)(v \otimes w) = (-1)^{|B||C| + (|B| + |D|)|v|}(ACv \otimes BDw)$$
so this is an algebra homomorphism. Then to show this is surjective, we use the same strategy as with the regular tensor product. Fix bases $\set{v_i}$ and $\set{w_j}$ such that they are all homogeneous elements of $V$ and $W$ (i.e. obtained by fixing bases for the even and odd subspaces and concatenating them). Then again, we want to be able to map $v_i \otimes w_j$ to any $v_k \otimes w_\ell$ and the rest to $0$. This has some subtleties regarding the signs of the basis vectors. In the case that $w_j$ and $w_\ell$ are the same parity, this requires $B$ to be even, so the sign will always be positive. Therefore, we can just pick $A$ to be the matrix that maps $v_i \mapsto v_k$ and the rest to $0$, and $B$ to be the matrix that maps $w_j \mapsto w_\ell$, and the rest to $0$. In the case that $w_j$ and $w_\ell$ have opposite parity, the $B$ is odd, and we need to check the cases for the parity of $v_i$. If $v_i$ is even, then again the sign will be positive, and we can do the same thing as the previous case. In the case that $v_i$ is odd, then there will be a negative sign. In this case, we do the same as before, except we replace $A$ with the map that maps $v_i \mapsto -v_k$, giving us the sign we desire. Finally, we want to check that this map respects the grading. Suppose $A \otimes B$ is even, i.e. $A$ and $B$ have the same parity. This then gives surjectivity.
%
\setcounter{section}{6}
%
\setcounter{thm}{0}
%
\section*{Week 6}
%
\begin{exer}
Give The universal property for the tensor product of
\begin{enumerate}
\item Algebras
\item Superalgebras
\end{enumerate}
Can you realize these tensor products as special cases of a more general construction?
\end{exer}
\begin{enumerate}
\item For $R$-algebras $A,B$, the tensor product $A \otimes B$ is another $R$-algebra equipped with a bilinear map $A \times B \to A \otimes B$ such that for any bilinear map $\varphi : A \times B \to C$ to another $R$-algebra $C$ satisfying 
$$\varphi((a,b)(c,d)) = \varphi(ac, bd)$$
we get a unique map $\tilde{\varphi} : A \otimes B \to C$ such that the diagram
$$\begin{tikzcd}
A \times B \ar[d] \ar[r, "\varphi"] & C \\
A \otimes B \ar[ur, "\tilde{\varphi}"']
\end{tikzcd}$$
\item For superalgebras $A,B$, we recall from before that the multiplication in $A \otimes B$ is defined on homogeneous elements as
$$(a_1 \otimes b_1)(a_2 \otimes b_2) = (-1)^{|b_1||a_2|} (a_1a_2 \otimes b_1b_2)$$
where $|b_1|$ denotes the parity of the $b_1$. Therefore, the universal property for super algebras needs the small modification that the map $\varphi A \times B \to C$ needs to satisfy
$$\varphi((a,b)(c,d)) = (-1)^{|b||c|}\varphi(ac,bd) $$
in order to factor through the tensor product $A \otimes B$.
\end{enumerate}
%
\begin{exer}
Using the notation that $C_{p,q}$ is the Clifford algebra of $\R^{p|q}$, we know that if $C_{p,q}$ is a matrix algebra, then $C_{p+1,q+1}$ is a matrix algebra. Is the converse true?
\end{exer}
%
\begin{exer}
For a Clifford algebra $C_{p,q}$, can you identify the even subalgebra $C_{p,q}^0$ as a different Clifford algebra? If so, is the isomorphism with or without grading?
\end{exer}
We note that for $C_{p,q}$, the even and odd subspaces are the same dimension, so the even subalgebra is an algebra of half the dimension of $C_{p,q}$, which has dimension $2^{p + q}$. This leads us to expect that it should be isomorphic to the Clifford algebra $C_{p-1,q}$ or $C_{p,q-1}$. By definition, it will be generated by all pairwise products of the $e_i$. From a simple computation, we get that
$$(e_ie_j)^2 = e_ie_je_ie_j = -e_i^2e_j^2 $$
giving us that 
$$(e_ie_j)^2 = \begin{cases} 
1 & e_i^2 \neq e_j^2 \\
-1 & e_i^2 = e_j^2
\end{cases}$$
For a basis vector $e_i$, denote it as $e_i^+$ or $e_i^-$ depending on whether it squares to plus or minus one, writing $C_{p,q}$ as generated by the basis vectors $e_1^+ \ldots e_p^+$ and $e_1^- \ldots e_q^-$. We have that a generating set for the even subspace (assuming $q \neq 0$) is the set 
$$\set{e_1^-e_j^+ ~:~ 1 \leq j \leq p} \cup \set{e_1^-e_k^- ~:~ 2 \leq k \leq q}$$
Noting that the elements in the first set square to $1$ and the elements in the second set square to $-1$, we get that the even subspace will be isomorphic to $C_{p,q-1}$ via the mappings
$$e_1^-e_j^+ \mapsto e_j^+ \qquad e_1^-e_k^- \mapsto e_{k-1}$$
We note in the special case that $p=1$, this gives us $C_{0,q}^0 \cong C_{0,q-1}$. Another equally good generating set would be
$$\set{e_1^+e_j^- ~:~ 1 \leq j \leq q} \cup \set{e_1^+e_k^+ ~:~ 2 \leq k \leq p} $$
where we now have that elements of the first set square to $-1$ and the elements of the second set square to $1$. Via a similar mapping, we get that $C_{p,q}^0 \cong C_{q,p-1}$. As a corollary, this gives us that $C_{p,q} \cong C_{q+1,p-1}$. 
%
\begin{exer}
For an algebra $A$, an ideal $I \subset A$ is a subalgebra such that for any $b \in I$, $ab \in I$ for all $a \in A$. An algebra $A$ is said to be \ib{simple} if it admits no nontrivial ideals i.e. the only ideals are $0$ and $A$. What are ideals in the Clifford algebra? Which Clifford Algebras are simple?
\end{exer}
We first have a nice lemma
%
\begin{lem*}
Let $A$ be a division algebra. Then the algebra $M_nA$ of matrices with entries in $A$ is a simple algebra.
\end{lem*}
%
\begin{proof}
Let $I \subset M_nA$ be an ideal that is not the $0$ ideal. Then let $M \subset I$ be a nontrivial matrix., which must have some entry $M^i_j \neq 0$. Then let $T$ be the matrix where
$$T^k_\ell = \begin{cases} 
(M^i_j)\inv & k = j, \ell= i \\
0 & \text{otherwise}
\end{cases} $$
Then $MT$ will be the matrix with $1$ in the $(i,i)$ entry and $0$ elsewhere. We can then use elementary matrices to multiply with $MT$ to be a $1$ in any of the diagonal entries, which implies that the identity matrices is in our ideal. Therefore, any nontrivial ideal must be all of $M_nA$, so $M_nA$ is simple.
\end{proof}
%
\begin{exer}
If we have an ideal $I$ of a superalgebra $A$, what conditions do we need to put on $I$ such that the quotient $A/I$ is also a super algebra?
\end{exer}
We note that the grading of a vector space $V = V^0 \oplus V^1$ is equivalent to the existence of an operator $\varepsilon : V \to V$ such that $\varepsilon\vert_{V^0} = \id_{V^0}$ and $\varepsilon\vert_{V^1} = -\id_{V^1}$. Given any such operator, we recover the decomposition as the $\pm 1$ eigenspaces. In the case of a superalgebra, we want the grading operator to respect the other algebra structure, i.e. if we have $A$ graded as $A = A^0 \oplus A^1$, we want
$$A^iA^j = A^{i + j \text{ mod } 2} $$
This translates to $\varepsilon$ being a map of algebras, rather than just a linear map, i.e. $\varepsilon(ab) = \varepsilon(a)\varepsilon(b)$. Therefore, if we want $A/I$ to have a grading compatible with the multiplication, we want $\varepsilon$ to descend to a map on the quotient $\tilde{\varepsilon} : A/I \to A/I$, i.e.
$$\begin{tikzcd} 
A \ar[d] \ar[r, "\varepsilon"] & A \ar[d] \\
A /I \ar[r, "\tilde{\varepsilon}"'] & A/I
\end{tikzcd}$$
commutes. From the universal property of the quotient, this amounts to requiring that $\varepsilon(I) \subset I$, and since $\varepsilon$ is an isomorphism, this is equivalent to $\varepsilon(I) = I$.
%
\begin{exer}
The center for an algebra $A$ consists of all the elements $a \in A$ that commute with all the other elements. Is there a different notion for superalgebras? If so, what?
\end{exer}
%
\begin{exer}
For an algebra $A$, the \ib{opposite algebra} $A^\text{op}$ an algebra define by the same underlying vector space, but the multiplication $a*b$ in $A^\text{op}$ is instead $ba$ (where this product is in $A$). Does this construction work for superalgebras? What is $C_{p,q}^\text{op}$?
\end{exer}
The idea needs a slight modification for the case of superalgebras. Since we're commuting two elements past each other in the multiplication, we want $a*b = (-1)^{|a||b|}ba$. Defining it this way, we investigate $C_{p,q}^\text{op}$, where again, we use $*$ to denote the multiplication in the opposite algebra.

We note that a basis $e_1^+ \ldots e_p^+, e_1^- , \ldots e_q^-$ for $\R^{p|q}$ is still a generating set for the the opposite algebra, so all that needs to be done is to investigate how the relations change. Our first observation is that $e_i^\pm * e^i_\pm = -(e_i^\pm)^2$, so now all of the positive basis elements square to negative one, and all the negative basis elements square to positive ones. This motivates us to suspect that the opposite algebra is in fact $C_{q,p}$ under the obvious map $e_i^\pm \mapsto e_i^\mp$
%
\begin{exer}
Recall from earlier that we constructed a subgroup of the multiplicative group of $C_{p,q}$ generated by the unit vectors in $\R^{p|q}$. Call this group $\Pin(p,q)$, and define $\Spin(p,q) = \Pin(p,q) \cap C_{p,q}^0$. For small values of $p,q$, identify these groups. (Hint : They will be Lie groups).
\end{exer}
%
\setcounter{thm}{0}
%
\setcounter{section}{7}
%
\section*{Week 7}
%
\begin{exer}
One way to construct the tensor product $A \otimes B$ of algebras is as a quotient of a free algebra modulo ideals giving us relations that we want. However, we already have the tensor product of vector spaces, so all that remains is to give an algebra structure on the underlying vector space $A \otimes B$. How do you do this?
\end{exer}
We have that an algebra $A$ is the data of a vector space $A$, along with a distinguished element $1_A$, and a linear map $\varphi : A \to \End(A)$ such that $\varphi(1_A) = \id_A$. This gives us the algebra multiplication $ab = \varphi(a)(b)$. We can then use this to construct an algebra structure on the vector space $A \otimes B$. Let $j : A \times B \to A \otimes B$ be the bilinear map $A \times B \to A \otimes B$ where $(a,b) \mapsto a \otimes b$, and let $1_A,1_B$ and $\varphi_A,\varphi_B$ be the units and maps to the endomorphism algebras for $A$ and $B$ respectively. Then let $1_{A \otimes B}$ by $j(1_A, 1_B) = 1_A \otimes 1_B$, and let $\varphi_{A \otimes B}$ be the map $a \otimes b \mapsto \varphi_A \otimes \varphi_B \in \End(A) \otimes \End(B)$, which is canonically isomorphic to $\End(A \otimes B)$. We note that $1_A \otimes 1_B \mapsto \id_A \otimes \id_B = \id_{A \otimes B}$, so this map defines the algebra structure on $A \otimes B$.
%
\begin{exer}
We constructed Clifford algebras where the base vector spaces are $\R$-vector spaces. The same construction (with minor modifications) can be made over $\C$. Can you also do this for $\mathbb{H}$? Do similar isomorphism theorems hold? Is there a relation between the real Clifford algebras and the complex ones?
\end{exer}
\begin{rem*}
Another fun thing to think about, can you construct Clifford algebras over $R$-modules for arbitrary rings $R$?
\end{rem*}
%
\begin{exer}
We've proven that $C_{p,q}^{\text{op}} \cong C_{q,p}$. Then note that $C_{p,q} \otimes C_{q,p} \cong C_{p + q, p + q}$, which is a matrix algebra, which will be isomorphic to $\End((\R^{1|1})^{\otimes n})$. However, this isomorphism is not very natural, since it will implicitly need a choice of basis. Is there a more natural vector space that $C_{p,q} \otimes C^\text{op}_{p,q}$ acts on?
\end{exer}
We have that the dimension of $C_{p+q, p+q}$ is $2^{2(p+q)}$, so we need the vector space to be $2^{p+q}$ dimensional. In addition, since we know that the even and odd subspaces need to have the same dimension. Conveniently, $C_{p,q}$ satisfies both of these properties. In addition, both $C_{p,q}$ and $C_{p,q}^\text{op}$ have natural actions on $C_{p,q}$ (namely multiplication), so all the evidence points us to try to find a map $\varphi: C_{p,q} \otimes C_{p,q}^\text{op} \to \End(C_{P,q})$ (where $\End$ denotes the space of linear maps, not the space of algebra endomorphisms. Define $\varphi$ on homogeneous elements of $C_{p,q} \otimes C_{p,q}^\text{op}$ by its action on homogeneous elements of $C_{p,q}$ (i.e even and odd elements) 
$$\varphi(a \otimes b)(v) = (-1)^{|b||v|} a(b*v)$$
where $*$ denotes the multiplication in $C_{p,q}^\text{op}$ and extending linearly to sums. The sign is motivated by the fact that the opposite multiplication in commutes $b$ and $v$. We then verify that this is an algebra map. We have that 
\begin{align*}
\varphi((a_1\otimes b_1)(a_2 \otimes b_2))(v) &= (-1)^{|b_1||a_2|}\varphi(a_1a_2 \otimes b_1*b_2)(v) \\
&= (-1)^{|b_1||a_2| + |b_1||b_2|}\varphi(a_1a_2 \otimes b_2b_1)(v) \\
&= (-1)^{|b_1||a_2| +|b_1||b_2| + |b_2b_1||v|}a_1a_2vb_2b_1 \\
&= (-1)^{|b_1||a_2| + |b_1||b_2| + |b_2||v| + |b_1||v|} a_1a_2vb_2b_1
\end{align*}
We then compute
\begin{align*}
(\varphi(a_1 \otimes b_1) \circ \varphi(a_2 \otimes b_2)(v) &= (-1)^{|b_2||v|} \varphi(a_1 \otimes b_1)(a_2vb_2) \\
&= (-1)^{|b_1||a_2| + |b_1||b_2| + |b_2||v| + |b_1||v|} a_1a_2vb_2b_1
\end{align*}
So they are equal, so $\varphi$ is an algebra map. We then need to check that it is a superalgebra map, i.e. it preserves grading. We observe for $a \otimes b$, 
$$|\varphi(a\otimes b)(v)| = |avb| = |a| + |v| + |b| $$
Therefore, if $a$ and $b$ are the same parity (i.e. $a \otimes b$ is even), then this is the parity of $v$, so $\varphi(a \otimes b)$ is also even. If $a$ and $b$ are different parities, then the parity of $|avb|$ will be the opposite, so $\varphi(a \otimes b)$ will be odd, just like $a \otimes b$. This gives us that $\varphi$ preserves the gradings, so it is a superalgebra map. Then since $C_{p,q} \otimes C_{p.q}^\text{op}$ is a matrix algebra, it is simple. Then since the kernel of the $\varphi$ is an idea and $\varphi$ is clearly not $0$, it must have trivial kernel, and since the domain and codomain it the same dimension, we have that $\varphi$ must be an isomorphism. Therefore, we have a canonical isomorphism
$$\varphi : C_{p,q} \otimes C_{p,q}^\text{op} \to \End(C_{p,q}) $$
%
\begin{exer}
Continue thinking about properties of $\Spin(p,q)$ and $\Pin(p,q)$. Continuing on last week, identify what low dimensional Lie groups they're be isomorphic to. Are they compact? Connected? What is $\pi_0(\Pin(p,q))$?
\end{exer}
We first begin by identifying the orthgonal $O(p,q)$ for $\R^{p|q}$ and its Lie algebra. Let $B$ denote the bilinear form for $\R^{p,q}$, and let $M$ denote its matrix in the standard basis, i.e. the diagonal matrix with $p$ ones the the diagonal and $q$ negative ones. Then we have that $B(v,w) = v^TMw$, where $v$ and $w$ are written in the standard coordinates. Then we have that 
$$O(p,q) = \set{A \in GL(p+q,\R) ~|~ v^TMw = v^TA^TMAw ~ \text{for all } v,w \in \R^{p,q}} $$
We note that in particular, this is equivalent to 
$$O(p,q) = \set{A \in GL(p+q, \R) ~:~ A^TMA = M} $$
Since $M$ will have determinant $\pm 1$, this implies that $(\det A)^2 = \pm 1$ for all $A \in O(p,q)$, so it will again have $2$ components. To compute the Lie algebra $\mathfrak{o}(p,q)$, we let $\varphi : GL(p+q, \R) \to M_{p + q}$ by $\varphi(X) = X^TMX$, which will be a constant rank map (since it is equivariant with respect to appropriate group actions by multiplication), so the kernel $\ker d\varphi_I$ will be the Lie algebra $\mathfrak{o}(p,q)$. To compute the differential, we prove $\varphi$ with curves based at $I$, and compute for $X \in M_{p+q}\R$
\begin{align*}
\varphi(tX + I) &= (tX + I)^T M (tX + I) \\
&= t^2X^TMX + t(MX + X^TM) + M
\end{align*}
which has linear term $MX + X^TM$, so $d\varphi_I(X) = MA + A^TM$. Therefore, we get that 
$$\mathfrak{o}(p,q) = \set{X \in M_{p+q}\R ~:~ MX = -X^TM} $$
an easy application on this identity shows that $\mathfrak{o}(p,q)$ is closed under the commutator bracket, so we get that $\mathfrak{o}(p,q)$ is a Lie algebra, as expected. In order to find out more about $\Pin(p,q)$ and $\Spin(p,q)$, we want to embed $\mathfrak{o}(p,q) = \mathfrak{so}(p,q)$ into $C_{p,q}$ with the commutator bracket (since we suspect that this is the Lie algebra for the group of units $C_{p,q}^\times$), and see what happens with the exponential map. 

To do this, we're going to assume some things we haven't fully proven yet, namely that $\Pin(p,q)$ and $\Spin(p,q)$ are smooth submanifolds of $C_{p,q}$, so we can compute the compute the differential of the map
\begin{align*}
\varphi : \Pin(p,q) &\to O(p,q) \\
\varphi(a)(v) &= ava^T
\end{align*}
where $a^T$ is the map that reverses the order of products, extended linearly to the entire algebra. We want to compute $d\varphi_1$. To do this, we have that 
\begin{align*}
\varphi(ta + 1)(v) &= (ta + 1)v(ta^T + 1) \\
&= t^2ava + tav + tva^T + v
\end{align*}
which has linear term $av + va^T$. So $d\varphi_1(A) = X$ where $\varphi_X = Xv+  vX^T$. We know that since this maps onto $\mathfrak{o}(p,q)$ that in particular this requires $Xv + vX^T \in \R^{p|q}$ for all $v \in \R^{P|q}$. We also note that that dimension of $\mathfrak{o}(p,q)$ is $n(n-1)/2 = \binom{n}{2}$, so we're looking for a certain number of linearly independent algebra elements satisfying this relation. We note that the set $\set{e_ie_j}$ of pairwise products works (and is the right number), since 
$$e_ie_jv + ve_je_i = 4B(e_j,v)e_i \in V$$
So we expect this to span the Lie algebra of $\Pin(p,q)$. From another piece of guess work, we want to formally exponentiate these elements, i.e. 
$$e^a = \sum_{i = 0}^\infty \frac{a^n}{n!} $$
If we look at the elements $e_ie_j$, there are a few cases to consider. In the case that $e_i^2 = e_j^2$, we have that $(e_ie_j)^2 = e_ie_je_ie_j = -e_i^2e_j^2 = -1$. In that case, we first note that 
\begin{align*}
(e_ie_j)^2  &= -1 \\
(e_ie_j)^3 &= -e_ie_j \\
(e_ie_j)^4 &= -(e_ie_j)^2 = 1 \\
(e_ie_j)^6 &= e_ie_j
\end{align*}
Then rearranging the terms in the sum, we get 
$$e^{te_ie_j} = \left(\sum_{k = 1}^\infty \frac{(-1)^{k+1}t}{2k !} \right) + \left(\sum_{k = 1}^\infty \frac{(-1)^{k+1}te_ie_j}{(2k-1)!} \right) = \cos t + (\sin t) e_ie_j$$
In the case that $e_i^2 \neq e_j^2$, we have that $(e_ie_j)^2 = 1$, which gives us
\begin{align*}
(e_ie_j)^2 &= 1 \\
(e_ie_j)^3 &= e_ie_j
\end{align*}
So the series becomes
$$e^{te_ie_j} = \left( \sum_{k=1}^\infty \frac{t}{2k !} \right)  + \left( \sum_{k = 1}^\infty \frac{te_ie_j}{(2k - 1)!}\right) = \cosh t + (\sinh t)e_ie_j$$
which are some very nice formulas indeed!
%
\begin{exer}
Continue thinking about past problems. Construct the table of Clifford algebras, continue progress on identifying matrix algebras, determining which Clifford algebras are simple, etc.
\end{exer}
%
\setcounter{section}{8}
%
\setcounter{thm}{0}
\section*{Week 8}
%
\begin{exer}
Last week, we constructed an isomorphism $C_{p,q} \otimes C_{p,q}^\text{op} \to \End(C_{p,q})$. In general, there always exists a map $A \otimes A^\text{op} \to \End(A)$ given by the same map. However, this does not always define an isomorphism. What is the obstruction to this being an isomorphism? Try out on the case that $A = \R \times \R$.
\end{exer}
%
\begin{exer}
The Lie algebra is supposed to be something along the lines ``The even subalgebra of the $2$-filtered subalgebra minus the scalars." Make sense of this.
\end{exer}
%
\begin{exer}
From before, we posed the question as to whether $C_{p+1,q+1}$ being a matrix algebra implied that $C_{p,q}$ was one. A more general question is that if we had that $A \otimes \End(V) \cong \End(W)$, can we conclude that $A$ is a matrix algebra? 
\end{exer}
%
We first prove some things about modules over some endmorphism algebra $\End(V)$. We have that the standard action of $\End(V)$ on $V$ gives $V$ the structure of an $\End(V)$-module, and that this module is irreducible (which we use interchangeably with simple), since the orbit of any vector $v \in V$ is all of $V$. We claim that this is the only irreducible $\End(V)$ module up to isomorphism. To see this, consider $\End(V)$ as a module over itself, with the algebra action given by left multiplication. Then if we fix a basis for $V$, we get isomorphisms $V \cong \R^n$ and $\End(V) \cong M_n\R$. Then we have a chain of left ideals $0 = I_0 \subset I_1 \subset I_2 \subset \ldots \subset  I_n$, where $I_k$ is the set of matrices with the first $k$ columns possibly nonzero, and the rest all zero. In particular, we have that the module $I_k / I_{k-1}$ is isomorphic to the module $V$, which is irreducible.

Then let $W$ be an arbitrary irreducible $\End(V)$-module. If we fix $w \in W$, then we get a module map $\varphi : \End(V) \to W$ where $\varphi(M) = M \cdot w$, where the right hand side is the algebra action on $W$. In particular, we have that the image of $\varphi$ will be a submodule of $W$, which is necessarily $0$ or all of $W$ since $W$ is irreducible. Since $\varphi$ is not the zero map, we conclude that $\varphi$ is surjective, so $W$ is a quotient of $\End(V)$ as an $\End(V)$ module. Then we know that there must exist some smallest $0 < k < n$ such that the ideal $\varphi(I_k) \neq 0$. Then since $I_{k-1}$ is mapped to $0$, $\varphi$ factors through the quotient to give a nonzero map $I_k / I_{k-1} \to W$. Then since $I_k / I_{k-1}$ is irreducible, and the map is nonzero, this map must be an isomorphism. Therefore $W$ is isomorphic to the standard representation $V$.

Another fact that we want to prove is that given any (associative and unital) $\R$-algebra $A$ and $U$ a simple $A$-module, we get that $U \otimes V$ is a simple $A \otimes \End(V)$ module. 
%
\begin{lem*}
Given an algebra $A$ and an irreducible $A$-module $U$, for any $u,u' \in U$, there exists $\alpha \in A$ such that $\alpha \cdot u = u'$.
\end{lem*}
%
\begin{proof}
We have that $A \cdot u$ is a submodule of $U$ that is nonzero, so it must be all of $U$. Therefore, we must have some $\alpha \in A$ such that $\alpha \cdot u = u'$.
\end{proof}
By fixing a basis for $V$, we get isomorphisms $U \otimes V \cong U \otimes \R^n$ and $A \otimes \End(V) \cong A \otimes M_n\R$. Then given an arbitrary elements $u^i \otimes e_i$ and $(u^i)' \otimes e_i$ (where the $e_i$ are the standard basis vectors for $\R^n$), Then let $\alpha_i \in A$ where $\alpha^i \cdot u^i = (u^i)'$ and let $E_{ii}$ denote the matrix with only a $1$ in the $(i,i)$ entry and zeroes elsewhere. Then $\alpha^i \otimes E_{ii} \cdot u^i \otimes e_i = (u^i)' \otimes e_i$, so $U \otimes V$ is simple, since the algebra action is transitive.

In the other direction, we want to show that given a simple $A \otimes \End(V)$-module $W$, it is isomorphic to a tensor product $U \otimes V$ for some irreducible $A$-module $U$. We know we have isomorphisms $A \cong A \otimes \id$ and $\End(V) \cong 1_A \otimes \End(V)$, which makes $W$ simultaneously an $A$-module as well as an $\End(V)$-module. Then by restricting to smaller and smaller subspaces if necessary, we can find a simple $A$-submodule $U \subset W$. Then we can endow an $\End(V)$-module structure on $\hom_A(U,W)$ where for $F \in \hom_A(U,W)$, the action of $M \in \End(V)$ on $F$ is given by
$$ (M \cdot F)(u) = (1_A \otimes M) \cdot F(u) $$
We then get a module homomorphism (as $A \otimes \hom_A(U, W)$-modules) given by evaluation
\begin{align*}
\varepsilon : U \otimes \hom_A(U,W) &\to W \\
u \otimes F \mapsto F(u)
\end{align*}
Then since $W$ is a simple module and the image is nonzero, the map $\varepsilon$ is necessarily surjective. In addition, we can (by shrinking until we find it) find a simple $\End(V)$-submodule $S \subset \hom_A(U,W)$, which will be isomorphic to $V$. Therefore, we have that $U \otimes S \subset U \otimes \hom_A(U,W)$ is a simple $A \otimes \End(V)$-module. We then observer that for any nonzero $\varphi \in \hom_A(U,W)$, the image $\varepsilon (U \otimes \spn{\varphi})$ is a nonzero subspace of $W$, so the restriction of $\varepsilon$ to the submodule $U \otimes S$ is a nonzero map, which must be surjective since $W$ is simple. Then since $U \otimes S$ is simple, it's also injective, so $U \otimes S \cong W$, so $W$ is isomorphic to the tensor product of $U$ with the standard representation $V$. Consequently, all the simple $A \otimes \End(V)$-modules are exactly tensor products of simple $A$-modules with the standard representation $V$.

With appropriate choices of sign, as well as the choice of basis being a homogeneous one, (i.e. a basis for the even subspace concatenated with a basis for the odd subspace) the proof mostly carries over for modules over superalgebras.

To summarize, the results we have are
\begin{enumerate}
\item The only irreducible representations (simple modules) of the algebra $\End(V)$ are isomorphic to $V$ with the standard action.
\item For any algebra $A$, the simple $A \otimes \End(V)$-modules are exactly tensor products of simple $A$-modules with $V$.
\end{enumerate}
We then want to use the results to answer the question in the affirmative -- if $A \otimes \End(V) \cong \End(W)$, then $A$ itself is an endomorphism algebra. We know that since the only simple $\End(W)$-module is $W$, we can decompose $W$ as a tensor product $U \otimes V$, where $V$ is the standard representation for $\End(V)$ and $U$ is a simple $A$-module, and $A$ acts on $U \otimes V$ as $A \otimes \id$. The module structure is induced by an algebra homomorphism $A \otimes \End(V) \to \End(U) \otimes \End(V)$, and since $A$ acts as $A \otimes \id$, we have that this gives us an algebra map $A \to \End(U)$. Since this is nonzero, and $\End(U)$ is simple, this map is necessarily surjective, and then comparing the dimensions using the isomorphism $A \otimes \End(V) \cong \End(W) \cong \End(U) \otimes \End(V)$, we have that this map must also be injective, so $A \cong \End(U)$.
%
\begin{exer}
The next few problems requires some knowledge of complex Clifford algebras. Construct and explore them.
\end{exer}
%
We note that there can't be a distinction $C_{p,q}$ when the ground field is $\C$, since if something squares to $-1$, we can multiply it by $i$ to make it square to $1$. Therefore, we only need to consider the Clifford algebra $C_n$, which is generated by $\C^n$ with the standard Hermitian bilinear form. We note that the first few explicit isomorphisms we constructed earlier all satisfy the Clifford relation, and will certainly satisfy that relation if we extend the field of scalars to $\C$, so this gives us that $C_n \cong C_{n,0} \otimes \C \cong C_{0,n} \otimes \C$. In addition, all of the isomorphisms we exhibited in the real case hold for the complex Clifford algebras as well, since all of the isomorphisms we gave still preserve the Clifford relations in the complex case. 
%
\begin{exer}
Isomorphism classes of left $A$-modules, denoted $[ _A \mathsf{Mod} ]$ form a commutative monoid under direct sum. In the case of $C_{p,q}$, what monoid is this isomorphic to? First try this with complex Clifford algebras
\end{exer}
We can answer this in the case that $C_{p.q}$ is a matrix algebra. First, we make a claim regarding the structure of all modules over an endomorphism algebra.
%
\begin{thm}
Given a finite dimensional $\End(V)$-module $M$, we can decompose $M$ as 
$$M \cong \bigoplus_{i=1}^n V $$
where $V$ is the standard representation of $\End(V)$.
\end{thm}
%
\begin{proof}
Let $M$ be an arbitrary (finite-dimensional) $\End(V)$-module $M$, and fix a basis for $V$, giving an isomorphism $\End(V) \cong M_n\R$. Again, let $E_{ii}$ denote the matrices with a $1$ in the $(i,i)$ entry and zero elsewhere.
\TODO
\end{proof}
%
\begin{exer}
Given an algebra map $\varphi: A \to B$, this induces a map $[ _B \mathsf{Mod}]$, where given any left $B$-module $M$, we can derive an $A$-module structure by $a \cdot m = \varphi(a) \cdot m$. In particular, we have an inclusion $C_{p,q} \hookrightarrow C_{p,q+1}$, which will give us a pullback map $[ _{C_{p,q+1}}\mathsf{Mod} ] \to [ _{C_{p,q}}\mathsf{Mod}]$. We can compute the cokernel of this map, which tells us the degree to which $C_{p,q}$-modules fail to extend $C_{p,q+1}$-modules. Again, first try this for complex Clifford Algebras.
\end{exer}
%
\setcounter{thm}{0}
%
\setcounter{section}{9}
%
\section*{Week 9}
%
\begin{exer}
We proved that all simple $A \otimes \End(V)$-modules are tensor products of simple $A$-modules with $V$. Prove a more general version -- simple $A \otimes B$-modules are exactly tensor products of simple $A$-modules with simple $B$-modules.
\end{exer}
%
\begin{exer}
Prove the following identity for extension of scalars for an algebra $A$ over a field $\F$.
$$\End_A(A \otimes_\F V) \cong A \otimes \End_\F(V) $$
\end{exer}
%
\begin{exer}
We now now that $C_{0,8} \cong \End(\R^{8|8})$. Play around a little with this, perhaps by finding the matrices that represent the standard basis elements.
\end{exer}
%
\begin{exer}
A \ib{linear category} is a category $\mathscr{C}$ such that each Hom-set $\hom_{\mathscr{C}}(A,B)$ has the structure of a vector space and composition is a bilinear map. Recognize the category $ _{\End(V)}\mathsf{Mod}$ as some other linear category.
\end{exer}
We'll need a lemma 
\begin{lem*}
In the category of vector spaces, the coproduct is direct sum, which coincides with the product in the finite case.
\end{lem*}
\begin{proof}
\TODO
\end{proof}
We now make several claims about the structure of the category $ _{\End(V)}\mathsf{Mod}$. Using the fact that we know that all (finite dimensional) $\End(V)$-modules are direct sums of the standard representation $V$.
%
\begin{conj}
In the category $ _{\End(V)}\mathsf{Mod}$, we have that as vector spaces,
$$\hom_{\End(V)}(V^{\oplus n}, V^{\oplus m}) \cong \R^{nm} $$
\end{conj}
%
\begin{proof}
Using the universal property of the coproduct, we have that any linear map $\varphi : V^{\oplus n} \to V^{\oplus m}$ is equivalent to giving $n$ linear maps $\varphi_i : V \to V^{\oplus m}$, where
$$\varphi(v_1, \ldots v_n) = \varphi_1(v_1) + \ldots + \varphi_n(v_n) $$
Then using the universal property of the product, we have that the data of any such map $\varphi_i : V \to V^{\oplus m}$ is equivalent to giving $m$ linear maps $\varphi_i^j : V \to V$, where
$$\varphi_i(v) = (\varphi_i^1(v), \ldots , \varphi_i^m(v)) $$
Then the fact that $\varphi$ is an $\End(V)$-module homomorphism implies that given any $M \in \End(V)$, we have that $\varphi(Mv) = M\varphi(V)$. Then since we know that the action of $M$ on $V^{\oplus n}$ and $V^{\oplus m}$ is just the standard action on each component, this condition implies that for all $i,j$, we have 
$$\varphi^j_i(Mv) = M \varphi_i^j(v)$$
so all the $\varphi^j_i$ commute with every $M$ in $\End(V)$. In particular, this implies that they are all scalar matrices, since $\End(V)$ is central. Therefore, the data of an $\End(V)$-module homomorphism is equivalent to the choice of $nm$ scalars, so $\hom_{\End(V)}(V^{\oplus n}, V^{\oplus m}) \cong \R^{nm}$ as vector spaces.
\end{proof}
This motivates us to suggest that the category $\mathscr{C}$ that we are looking for is the subcategory of $\mathsf{Vect}_\R$ where the objects are the vector spaces $\R^n$ and the maps are linear maps $\R^n \to \R^m$ (i.e. matrices). We claim the assignment $\mathcal{F} : _{\End(V)}\mathsf{Mod} \to \mathscr{C}$ where $\mathcal{F}(V^{\oplus n}) = \R^n$ and $\mathcal{F}(\varphi)$ is the matrix $\varphi_i^j$ of scalars (as specified above) corresponding to $\varphi$ defines a functor. We note that the composition of such maps is essentially matrix multiplication, since each function $\varphi_i^j$ gives the scalar for a map from the $i^{th}$ factor to the $j^{th}$ factor of the direct sum, so after composing two maps $\varphi : V^{\oplus n} \to V^{\oplus m}$ and $\psi : V^{\oplus m} \to V^{\oplus \ell}$, we have that the scalar for the map from the $i^{th}$ factor of $V^{\oplus n}$ to the $j^{th}$ factor of $V^{\oplus \ell}$ is the sum over $k$ of the products of scalars for maps of the $i^{th}$ factor of $V^{\oplus n}$ into the $k^{th}$ factor of $V^{\oplus m}$ with the scalar for the map of the $k^{th}$ factor into the $j^{th}$ factor of $V^{\oplus \ell}$, which is exactly the matrix product.
%
\begin{exer}
Let $V$ be a $4$-dimensional $\C$-vector space, and $\omega \in \bigwedge^4 V^*$ a volume form (i.e. $\omega \neq 0$). Then we have the group $\Aut(V,\omega)$ of linear automorphisms of $V$ preserving $\omega$, which is equivalent to the induced maps on $\bigwedge^4V^*$ being identity. Identify this group.
\end{exer}
We recall that any linear map $T : V \to V$ induces a linear map $T^* : \bigwedge^4V^* \to \bigwedge^4V^*$ by pullback, where
$$T^*\eta(v_1,v_2,v_3,v_4) = \eta(Tv_1,Tv_2,Tv_3,Tv_4) $$
for any $\eta \in \bigwedge^4V^*$. We also note that $\bigwedge^4V^*$ is $1$ dimensional, since $V$ is $4$ dimensional, so $\bigwedge^4V^*$ is spanned by $\omega$, so every element is a scalar multiple of $\omega$. Then the unique scalar $\lambda$ where $T^*\omega = \lambda \omega$ must be $1$ for $T$ to preserve $\omega$. More explicitly, if we fix a basis $v_1, \ldots v_4$ such that $\omega(v_1,v_2,v_3,v_4) = 1$, we get a isomorphism $(V,\omega) \to (\R^n \det)$, where $\det$ denotes the standard volume form on $\R^n$. Then the condition that $T$ preserves $\omega$ is equivalent to the matrix representation of $T$ to have determinant $1$, so $\Aut(V, \omega) \cong SL_4(\C)$.
%
\begin{exer}
First, identify the dual pairing of $\bigwedge^4 V$ with $\bigwedge^4V^*$. Then the volume form $\omega$ will induce a bilinear map
$$B : \bigwedge^2V \times \bigwedge^2V \to \bigwedge^4V \to \C $$
where the first map is the mapping $(\alpha,\beta) \mapsto \alpha \wedge \beta$. Show that $B$ is symmetric and nondegenerate.
\end{exer}
%
We note the since $V$ is $4$-dimensional, $\bigwedge^4 V$ is $4$-dimensional, and consists of scalar multiples of 
$$\eta = v_1 \wedge v_2 \wedge v_3 \wedge v_4$$
for a basis $\set{v_i}$ for $V$. Therefore, for $\alpha,\beta \in \bigwedge^2V$ we have that $\alpha \wedge \beta = \lambda \eta$. We can say that this is just $\alpha \wedge \beta = \lambda v_1 \wedge v_2 \wedge v_3 \wedge v_4$, and then let $B(\alpha, \beta) = \omega(\lambda v_1, v_2, v_3, v_4)$. Then since $\omega$ is multilinear and the wedge product is bilinear, this is independent of how we chose to scale the $v_i$. In addition, this is independent of our choice of basis $\set{v_i}$, since it we fixed another basis $\set{b_i}$ the wedge $b_1 \wedge b_2 \wedge b_3 \wedge b_3$ is just the determinant of the change of basis matrix multiplied with $\eta$. Then the scalar you get for $\alpha \wedge \beta$ will be the reciprocal of this determinant, but since $\omega$ is multilinear, $\omega(b_1, b_2, b_3, b_3)$ also scales by the determinant, so the value remains the same.

 We then note that $\alpha \wedge \beta = (-1)^{|\alpha||\beta|} \beta \wedge \alpha$, and since both $\alpha$ and $\beta$ are $2$-forms, this gives us that $\alpha \wedge \beta = \beta \wedge \alpha$, so the bilinear form is certainly symmetric. For nondegeneracy. suppose $B(\alpha, \beta)$ is $0$ for all $\beta$. Then since $\omega$ is a volume form, this implies that the vectors $\lambda v_1 \wedge v_2 \wedge v_3 \wedge v_4$ determined by $\alpha \wedge \beta$ are linearly dependent, so $\lambda$ must be $0$ so $\alpha \wedge \beta = 0$ for all $\beta$, so $\alpha$ is necessarily $0$, so $B$ is nondegenerate, so it determines an inner product on $\bigwedge^2V$.
%
\begin{exer}
Building upon the previous question, we can endow $V$ with the additional structure of a Hermitian inner product $\langle \cdot, \cdot \rangle$. We can then consider $\Aut(V, \omega, \langle \cdot,\cdot \rangle)$ is a subgroup of $\Aut(V, \omega)$. In addition, a map $T \in \Aut(V)$ induces a map $T_* \in \Aut(\bigwedge^2V)$, which gives a map $\Aut(V) \to \Aut(\bigwedge^2V)$. Identify the images of all the subgroups. 
\end{exer}
One thing to note here is that if a linear map $T : V \to V$ preserves $\omega$, then the induced map $T_* : \bigwedge^2V \to \bigwedge^2V$ determined by $T_*(v \wedge w) = Tv \wedge Tw$ must necessarily preserve $B$. To see this, we note that after fixing a basis $v_1 \ldots v_4$ for $V$ such that $\omega(v_1, v_2, v_3, v_4) = 1$, this also gives us a basis for $\bigwedge^4V$, which we can use to compute the value of $B(\alpha,\beta)$ in coordinates. Then by checking on the basis for $\bigwedge^2V$ given by pairwise wedges $v_i \wedge v_j$, we have that 
$$T_*(v_i \wedge v_j) \wedge T_*(v_k \wedge v_\ell) = Tv_i \wedge Tv_j \wedge Tv_k \wedge Tv_\ell $$
which is just the determinant of $T$ multiplied by the wedge $v_i \wedge v_j \wedge v_k \wedge v_\ell$. Then since the determinant of $T$ is $1$ with respect to this basis, we have that 
$$\omega(Tv_i, Tv_j, Tv_k, Tv_\ell) = \det T \omega (v_1, v_2, v_3, v_3) = \omega(v_1,v_2,v_3,v_4)$$
So $T_*$ preserves $B$, giving us a homomorphism $\Aut(V, \omega) \to \Aut(\bigwedge^2V, B)$. We then want to compute the kernel. We certainly have that $\id$ and $-\id$ are in the kernel, and we claim that this is all of the kernel. Suppose $T \in \Aut(\omega)$ is in the kernel, i.e. $T_* = \id$. This implies that for all $v,w \in V$ we have that 
$$Tv \wedge Tw = v \wedge w $$
If we fix a basis $\set{e_i}$ for $V$, we have that $Te_i = v^ke_k$, so in particular, we must have that $Te_i \wedge Te_j = e_i \wedge e_j$. 
% Show that T_* = \id \iff T = \pm \id
\begin{TODO}
Show that $T_* = \id \iff T = \pm \id$
\end{TODO}

We can then add the additional structure of a Hermitian inner product $\langle \cdot,\cdot \rangle : V \times V \to \C$. This induces an inner product on $\bigwedge^2V$, which by abuse of notation we also refer as $\langle \cdot,\cdot \rangle$, which is uniquely determined by 
$$\langle v_i \wedge v_j, v_k \wedge v_l \rangle = \det \begin{pmatrix}
\langle v_i, v_k \rangle & \langle v_i v_\ell \rangle \\
\langle v_j, v_k \rangle & \langle v_j, v_\ell \rangle
\end{pmatrix} $$
%
\begin{TODO}
Show the induced inner product on $\bigwedge^2V$ is indeed a Hermitian inner product
\end{TODO}
We note that if a linear map $T : V \to V$ preserves the inner product on $V$, then $T_*$ clearly must preserve the induced inner product on $\bigwedge^2 V$. In addition, if $T$ preserves $\omega$, it must preserve $B$ as well. We then note that after fixing an appropriate basis, we have that $\Aut(V,\omega, \langle \cdot,\cdot \rangle)$ is isomorphic to $SL_4(\C) \cap U(4) = SU_4(\C)$, and that the subgroup of linear maps $\bigwedge^2 V \to \bigwedge^2V$ that preserve $B$ and $\langle \cdot,\cdot \rangle$ is isomorphic to $SO_6\C) \cap U(6)$. We note that for a matrix $A$ to be in $SO_6(\C)$, we must have that $A^T = A\inv$, where $T$ is just the transpose, without any complex conjugation. The condition that $A \in U(6)$ then gives us hat $A^\dagger = A\inv$, where $\dagger$ denotes the conjugate transpose. Therefore, we must have that $A^T = A^\dagger = A\inv$. Therefore, all the elements of $A$ must be real so $A^T$ and $A^\dagger$ coincide, which gives us that $A \in SO(6, \R)$, so we now get a homomorphism $SU_4 \to SO_6(\R)$. We again get that kernel must be $\pm \id$, giving us the two-fold covering $SU_4 \to SO_6(\R)$.
%
\setcounter{section}{10}
%
\setcounter{thm}{0}
%
\section*{Week 10}
%
\begin{exer}
From last week, we got a functor from the subcategory $\mathscr{R}$ of $\mathsf{Vect}_\R$ where the objects are just $\R^n$ for $n \geq 0$ to the category of modules $ _{\End(V)}\mathsf{Mod}$. In addition, there is an equivalence of categories $\mathscr{R} \to \mathsf{Vect}_\R$ (think about this). Give a more natural functor from $\mathsf{Vect}_\R  \to  _{\End(V)}\mathsf{Mod}$ than just taking the inverse functor to $\mathscr{R}$. More concretely, given an arbitrary vector space $W$, give a natural construction of an $\End(V)$-module from $W$.
\end{exer}
%
\begin{exer}
Last week, we gave two-fold covers $SL_4(\C) \to SO_6(\C)$ and $SU(4) \to SO_6(\R)$. Since these are covers, they are local diffeomorphisms. Find the induced maps of Lie algebras, and check that they are isomorphisms.
\end{exer}
%
\begin{exer}
Show that in a connected Lie group $G$ any discrete normal subgroup $N$ is necessarily central, i.e. $N \subset Z(G)$.
\end{exer}
%
\begin{exer}
We're going to change up the notation a bit to fit more with convention. Again let $V$ be a $4$-dimensional complex vector space, and fix a volume form $\mu \in \bigwedge^4V^*$. Then let $\omega \in \bigwedge^2V^*$ be a skew-symmetric form where $\omega \wedge \omega = 2\mu$. Show that this implies that $\omega$ is nondegenerate.
\end{exer}
We first fix a basis $e_i$ for $V$, which induces a dual basis $e^i$. Then we can write
$$\omega = \omega_{ij}e^i \wedge e^j $$
where $i < j$. Then we compute
\begin{align*}
\omega \wedge \omega &= (\omega_{ij} e^i \wedge e^j) \wedge (\omega_{k\ell} e^k \wedge e^\ell) \\
&= \omega_{ij}\omega_{k\ell} e^i \wedge e^j \wedge e^k \wedge e^\ell
\end{align*}
where $i < j$ and $k < \ell$. This is then equal to
$$\omega \wedge \omega = \omega_{12}\omega_{34} e^1\wedge e^2 \wedge e^3 \wedge e^4  + \omega_{13}\omega_{24} e^1 \wedge e^3 \wedge e^2 \wedge e^4  + \omega_{14}\omega_{23} e^1 \wedge e^4 \wedge e^2 \wedge e^3$$
which simplifies to
$$\omega \wedge \omega = (2\omega_{12}\omega_{34} - 2\omega_{13}\omega_{24} + 2\omega_{14}\omega_{23}) e^1 \wedge e^2 \wedge e^3 \wedge e^4 = 2\mu $$
By abuse of notation, we let $\omega_{ij}$ also define the matrix for $\omega$ with respect to the $e_i$, i.e $\omega_{ij} = \omega(e_i,e_j)$. We note that this implies that $\omega_{ij}$is skew-symmetric. We then compute that the coefficient for $\mu$ squares to
$$(\omega_{12}\omega_{34} - \omega_{13}\omega_{24} + \omega_{14}\omega_{23})^2 = \omega_{12}^2\omega_{34}^2 + \omega_{13}^2\omega_{24}^2 + \omega_{14}^2\omega_{23}^2 + 2\omega_{12}\omega_{34}\omega_{14}\omega_{23} - 2\omega_{12}\omega_{34}\omega_{13}\omega_{24} - 2\omega_{13}\omega_{23}\omega_{14}\omega_{24}$$
Where we use the skew-symmetry $\omega_{ij} = - \omega_{ji}$. Then when using the Leibniz formula to compute the determinant of $\omega_{ij}$, we get that any permutation $\sigma \in S_4$ contributes $0$ to the sum, since it will have $\omega_{kk} = 0$ in the product. Therefore, the only permutations that contribute to the sum are
\begin{align*}
&(1234) &&(1243) &&&(1324) \\
&(1342) &&(1423) &&&(1432) \\
&(12)(34) &&(13)(24) &&&(14)(23) 
\end{align*}
Expanding out these products and using skew-symmetry of $\omega_{ij}$, we find that this is exactly the square of the coefficient of $\mu$, so this gives us that $\omega \wedge \omega$ is a volume form iff $\omega$ is nondegenerate.
%
\begin{exer}
Following on the previous question, we now have a subgroup $\Aut(V, \omega) \subset \Aut(V, \mu)$. From before, we have maps $\Aut(V, \mu) \to \Aut(\bigwedge^2V, B)$, where $B$ is the bilinear from we got from $\mu$. Identify the subgroup $G$ that is the image of $\Aut(V, \omega)$ under this map. After fixing a basis, identify $G$ as a matrix group.
\end{exer}
We have that $\omega$ induces a bilinear form $\tilde{\omega}$ on $\bigwedge^2V$ (just like the Hermitian form last week), where we define on a basis $v_i$ 
$$\tilde{\omega}(v_i \wedge v_j, v_k) \wedge v_\ell = \det\begin{pmatrix}
\omega(v_i,v_k) & \omega(v_i,v_\ell) \\
\omega(v_j,v_k) & \omega(v_k, v_\ell)
\end{pmatrix} $$
However, this has the unexpected property of being symmetric, rather than skew symmetric like $\omega$ was. To see this, we compute
\begin{align*}
\tilde{\omega}(v_k \wedge v_\ell, v_i \wedge v_j) &= \det\begin{pmatrix}
\omega(v_k, v_i) & \omega(v_k, v_j) \\
\omega(v_\ell, v_i) & \omega(v_\ell, v_j)
\end{pmatrix} \\[5pt]
&= \det\begin{pmatrix}
-\omega(v_i, v_k) & -\omega(v_j, v_k) \\
-\omega(v_i, v_\ell) & -\omega(v_j, v_\ell)
\end{pmatrix} \\[5pt]
&= \tilde{\omega}(v_i \wedge v_j, v_k \wedge v_\ell)
\end{align*}
where the last equality comes from observing that we are taking the determinant of the negative transpose, and since multiplying an even number of rows by $-1$ scales the determinant by $(1)^4$ from the multilinearity of $\det$. In addition, a linear map $T \in \Aut(V,\omega)$ certainly satisfies that $T_* \in \Aut(\bigwedge^2V, \tilde{\omega}, B)$, so we need it to simultaneously preserve two bilinear forms. We recall that the formula for $B$ is given by
$$B(\alpha, \beta) = \langle \mu, \alpha \wedge \beta  \rangle $$
where $\langle \cdot,\cdot \rangle$ denotes the dual pairing of $\bigwedge^4 V^*$ and $\bigwedge^4V$. Fix a basis $\set{e_1, e_2, e_3, e_4}$ for $V$ such that 
\begin{align*}
\omega(e_1,e_2) &= 0 \\
\omega(e_1, e_3) &= 1 \\
\omega (e_1, e_4) &= 0 \\
\omega(e_2, e_3) &= 0 \\
\omega(e_2, e_4) &= 1
\end{align*}
i.e. a symplectic basis for $V$. We note that this implies that $\mu(e_1, e_2, e_3, e_4) = 1$. Then fix an ordered basis
$$(e_1 \wedge e_2, e_3 \wedge e_4, e_1 \wedge e_3, e_2 \wedge e_4, e_1 \wedge e_4, e_2 \wedge e_3) $$
for $\bigwedge^2V$. Then in this ordered basis, we have that $B$ is given by the matrix
$$B = \begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 & 0
\end{pmatrix} $$
When we compute the matrix for $\tilde{\omega}$, we get that 
$$\tilde{\omega} = \begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 0  \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 & 0
\end{pmatrix} $$
In block form, we have that 
\begin{align*}
B &= \begin{pmatrix}
\sigma_x & 0 & 0 \\
0 & -\sigma_x & 0 \\
0 & 0 & \sigma_x
\end{pmatrix} \\[5pt]
\tilde{\omega} &= \begin{pmatrix}
\sigma_x & 0 & 0 \\
0 & \id & 0 \\
0 & 0 & \sigma_x
\end{pmatrix}
\end{align*}
Where $\sigma_x$ is the Pauli matrix
$$ \sigma_x = \begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix} $$
We note that these two matrices commute, so they are simultaneously diagonalizable, which yields
\begin{align*}
B &= \begin{pmatrix}
-1 & 0 & 0 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 & 0 & 0 \\
0 & 0 & -1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1
\end{pmatrix} \\[5pt]
\tilde{\omega} &= \begin{pmatrix}
-1 & 0 & 0 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1
\end{pmatrix}
\end{align*}
%
\begin{exer}
Again expanding on the previous problem, there are several things to consider
\begin{enumerate}
\item Identify the image of the group $\Aut(V,\omega, \langle \cdot,\cdot \rangle)$, where $\langle \cdot, \cdot \rangle$ is a Hermitian inner product
\item Consider the case where $\langle \cdot,\cdot \rangle$ need not be positive definite, what about the mixed signatures $(3,1)$ and $(2,2)$?
\item What if we had done all of this in an $\R$-vector space rather than over $\C$?
\end{enumerate}
\end{exer}
%
\begin{exer}
With $V$ as above, we can also endow $V$ with additional structure by giving the data of an antilinear map $J : V \to V$ where $J^2 = \id_V$. In particular, $J$ fixes some subspace of $V$, and we can consider maps $T : V \to V$ that fix this subspace. Identify this group. Another choice is an antilinear map $J : V \to V$ where $J^2 = \id_V$. This gives us a quaternionic structure on $V$, and we can ask for maps that preserve this quaternionic structure. Find this group.
\end{exer}
%
\end{document}